{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ec27ce",
   "metadata": {},
   "source": [
    "# Trying to make a fit function for a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8a943",
   "metadata": {},
   "source": [
    "# making a simple neural net for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb31cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0                   4         88.944468             57.862692   \n",
       "1                   5         92.729214             58.518416   \n",
       "2                   4         88.944468             57.885242   \n",
       "3                   4         88.944468             57.873967   \n",
       "4                   4         88.944468             57.840143   \n",
       "\n",
       "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0          66.361592              36.116612             1.181795   \n",
       "1          73.132787              36.396602             1.449309   \n",
       "2          66.361592              36.122509             1.181795   \n",
       "3          66.361592              36.119560             1.181795   \n",
       "4          66.361592              36.110716             1.181795   \n",
       "\n",
       "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                 1.062396          122.90607              31.794921   \n",
       "1                 1.057755          122.90607              36.161939   \n",
       "2                 0.975980          122.90607              35.741099   \n",
       "3                 1.022291          122.90607              33.768010   \n",
       "4                 1.129224          122.90607              27.848743   \n",
       "\n",
       "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
       "0        51.968828  ...          2.257143       2.213364           2.219783   \n",
       "1        47.094633  ...          2.257143       1.888175           2.210679   \n",
       "2        51.968828  ...          2.271429       2.213364           2.232679   \n",
       "3        51.968828  ...          2.264286       2.213364           2.226222   \n",
       "4        51.968828  ...          2.242857       2.213364           2.206963   \n",
       "\n",
       "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
       "0         1.368922             1.066221              1           1.085714   \n",
       "1         1.557113             1.047221              2           1.128571   \n",
       "2         1.368922             1.029175              1           1.114286   \n",
       "3         1.368922             1.048834              1           1.100000   \n",
       "4         1.368922             1.096052              1           1.057143   \n",
       "\n",
       "   std_Valence  wtd_std_Valence  critical_temp  \n",
       "0     0.433013         0.437059           29.0  \n",
       "1     0.632456         0.468606           26.0  \n",
       "2     0.433013         0.444697           19.0  \n",
       "3     0.433013         0.440952           22.0  \n",
       "4     0.433013         0.428809           23.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "superconductivty_data = fetch_ucirepo(id=464)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = superconductivty_data.data.features\n",
    "y = superconductivty_data.data.targets\n",
    "\n",
    "df = X.join(y)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b838c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21263, 81)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1909ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split the SCALED data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97fbcfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 10:49:14.346206: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-12 10:49:14.359402: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-12 10:49:14.776866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-12 10:49:15.874924: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-12 10:49:15.877109: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f65cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-12-12 10:49:16.093138: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">665</span> (2.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m665\u001b[0m (2.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">665</span> (2.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m665\u001b[0m (2.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "num_cols = X.shape[1]\n",
    "\n",
    "# add hidden layer\n",
    "model.add(Dense(8, input_shape = (num_cols,), activation = 'relu'))\n",
    "\n",
    "# add output layer\n",
    "model.add(Dense(1, input_shape = (8,), activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2014c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15947, 81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc57e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_of_elements      -0.080058\n",
       "mean_atomic_mass         0.115185\n",
       "wtd_mean_atomic_mass    -0.407469\n",
       "gmean_atomic_mass       -0.127719\n",
       "wtd_gmean_atomic_mass   -0.601454\n",
       "                           ...   \n",
       "wtd_entropy_Valence      0.117916\n",
       "range_Valence           -0.033011\n",
       "wtd_range_Valence       -0.437128\n",
       "std_Valence             -0.021017\n",
       "wtd_std_Valence         -0.367332\n",
       "Name: 1351, Length: 81, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0f8136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04931911]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train.iloc[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5060fe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f1531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_params_list(length: int) -> list:\n",
    "    x: list(float) = []\n",
    "    for i in range(length):\n",
    "        x.append(random.uniform(-50, 50))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6e6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shape_model_params(params: list[float], params_shape: list[()]) -> list[np.ndarray[float]]:\n",
    "    shaped_params_list: list[np.ndarray[float]] = []\n",
    "    working_params: np.ndarray[float] = np.array(params)\n",
    "    for i in params_shape:\n",
    "        first_i_params_list: list[float] = (working_params[:np.prod(i)]).reshape(i)\n",
    "        working_params = working_params[np.prod(i):]\n",
    "        shaped_params_list.append(first_i_params_list)\n",
    "    return shaped_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b6988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import numpy as np\n",
    "\n",
    "def model_rmse(\n",
    "    model: Callable,\n",
    "    params: list[float],\n",
    "    params_shape: list[()],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame\n",
    ") -> float:\n",
    "    # Set weights\n",
    "    model.set_weights(shape_model_params(params=params, params_shape=params_shape))\n",
    "    \n",
    "    # Predict the whole batch at once (Vectorization)\n",
    "    # verbose=0 prevents Keras from printing a progress bar 1300 times\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "    \n",
    "    # Ensure shapes match for subtraction\n",
    "    # Keras returns shape (32, 1). y_train might be (32,). \n",
    "    # We flatten predictions to match y_train.\n",
    "    predictions_flat = predictions.flatten() \n",
    "    y_values = y_train.values.flatten() # Convert to numpy array\n",
    "    \n",
    "    # Calculate RMSE using Numpy (very fast)\n",
    "    mse = np.mean((predictions_flat - y_values) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f89413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def model_get_error_gradient_manual(\n",
    "    model: Callable,\n",
    "    params: list[float],\n",
    "    params_shape: list[()],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    error: Callable[[Callable], float]\n",
    ") -> list[float]:\n",
    "    gradient: list[float] = []\n",
    "    working_params: list[float] = params.copy()\n",
    "\n",
    "    for i in range(len(working_params)):\n",
    "        working_params[i] += -0.05\n",
    "        lower_error = error(model = model, params = working_params, params_shape = params_shape, X_train = X_train, y_train = y_train)\n",
    "        working_params[i] += 0.1\n",
    "        upper_error = error(model = model, params = working_params, params_shape = params_shape, X_train = X_train, y_train = y_train)\n",
    "        working_params[i] += -0.05\n",
    "        gradient.append(10 * (upper_error - lower_error))\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "444dbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def train_manual(\n",
    "    model: Callable,\n",
    "    num_params: int,\n",
    "    params_shape: list[()],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    epochs: int,\n",
    "    learning_rate: float = 0.01,\n",
    "    batch_size: int = 32,\n",
    "    error: Callable[[Callable], float] = model_rmse\n",
    ") -> list[float]:\n",
    "    # Runtime validation\n",
    "    for dtype in X_train.dtypes:\n",
    "        if not (np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.floating)):\n",
    "            raise TypeError(f\"DataFrames must only contain int or float values, but found {dtype}\")\n",
    "    for dtype in y_train.dtypes:\n",
    "        if not (np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.floating)):\n",
    "            raise TypeError(f\"DataFrames must only contain int or float values, but found {dtype}\")\n",
    "            \n",
    "    # Get random starting params\n",
    "    best_fit_params: list = get_random_params_list(num_params)\n",
    "\n",
    "    df_train = X_train.join(y_train)\n",
    "    num_samples = len(df_train)\n",
    "    total_batches = np.ceil(num_samples / batch_size)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(f\"--- Starting Epoch {i+1}/{epochs} ---\")\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        shuffled_df = df_train.sample(frac=1) \n",
    "        \n",
    "        # Split shuffled data back into X and y\n",
    "        X_shuffled = shuffled_df.iloc[:,:-1]\n",
    "        y_shuffled = shuffled_df.iloc[:,-1]\n",
    "\n",
    "        # This loop iterates through the shuffled data in chunks\n",
    "        # (e.g., from 0 to 999 in steps of 32)\n",
    "        for j in range(0, num_samples, batch_size):\n",
    "            \n",
    "            # Get the current batch\n",
    "            X_train_batch = X_shuffled.iloc[j : j + batch_size]\n",
    "            y_train_batch = y_shuffled.iloc[j : j + batch_size]\n",
    "\n",
    "            # Calculate gradient on this batch\n",
    "            gradient = model_get_error_gradient_manual(\n",
    "                model = model,\n",
    "                params = best_fit_params,\n",
    "                params_shape = params_shape,\n",
    "                X_train = X_train_batch, # Use the small batch\n",
    "                y_train = y_train_batch, # Use the small batch\n",
    "                error = error\n",
    "            )\n",
    "\n",
    "            # Calculate and print out the batch number/epoch progress\n",
    "            current_batch = (j // batch_size) + 1\n",
    "            print(f\"  Batch {current_batch}/{total_batches}\", end=\"\\r\", flush=True)\n",
    "\n",
    "            # Update parameters after every batch, not just once per epoch\n",
    "            best_fit_params = [x + y for x, y, in zip(best_fit_params, [n*learning_rate for n in gradient])]\n",
    "            \n",
    "            # Update best_fit_params:\n",
    "            best_fit_params = [x + y for x, y, in zip(best_fit_params, [n*-1 for n in gradient])]\n",
    "\n",
    "        # Print a blank line. This moves the cursor to the next line so the epoch message isn't overwritten\n",
    "        print()\n",
    "    \n",
    "    return best_fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ad3ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(\n",
    "#    model = model,\n",
    "#    num_params = 665,\n",
    "#    params_shape = [(81,8),(8,),(8,1),(1,)],\n",
    "#    X_train = X_train,\n",
    "#    y_train = y_train,\n",
    "#    epochs = 2,\n",
    "#    error = model_rmse\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff5fefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    np.float64(-46.94655716577822),\n",
    "    np.float64(-34.99845364740807),\n",
    "    np.float64(-38.976147905046346),\n",
    "    np.float64(-34.99494378446454),\n",
    "    np.float64(35.15852255674987),\n",
    "    np.float64(-28.64318630220728),\n",
    "    np.float64(-17.16844299856436),\n",
    "    np.float64(1.6263261752418003),\n",
    "    np.float64(-42.8093094543577),\n",
    "    np.float64(6.35945897781739),\n",
    "    np.float64(-5.4759399220478),\n",
    "    np.float64(36.512748858128134),\n",
    "    np.float64(26.04541461030645),\n",
    "    np.float64(-27.55815634171549),\n",
    "    np.float64(29.311976804966278),\n",
    "    np.float64(27.024303196913323),\n",
    "    np.float64(-16.272687811295995),\n",
    "    np.float64(18.561041810668428),\n",
    "    np.float64(41.16896116074224),\n",
    "    np.float64(34.24018933748218),\n",
    "    np.float64(34.552195061103774),\n",
    "    np.float64(-22.295802401298925),\n",
    "    np.float64(15.540756031419306),\n",
    "    np.float64(41.810894963067426),\n",
    "    np.float64(30.024838563848007),\n",
    "    np.float64(-32.68524777934783),\n",
    "    np.float64(25.900407833831707),\n",
    "    np.float64(22.9335571510333),\n",
    "    np.float64(-43.730228456084696),\n",
    "    np.float64(45.44700104291309),\n",
    "    np.float64(34.240642622897084),\n",
    "    np.float64(23.55562628815899),\n",
    "    np.float64(35.71122600704527),\n",
    "    np.float64(24.79305572877179),\n",
    "    np.float64(22.787309292405084),\n",
    "    np.float64(16.35851765207387),\n",
    "    np.float64(-39.97037791915386),\n",
    "    np.float64(-49.29749397558797),\n",
    "    np.float64(48.292416427783465),\n",
    "    np.float64(13.560424999606049),\n",
    "    np.float64(-10.615344136199056),\n",
    "    np.float64(38.94095367222819),\n",
    "    np.float64(-45.876680415228364),\n",
    "    np.float64(6.960959254323086),\n",
    "    np.float64(-32.443313450473056),\n",
    "    np.float64(34.65932044090624),\n",
    "    np.float64(-27.799512251204995),\n",
    "    np.float64(-39.73922360784595),\n",
    "    np.float64(-10.377553233113169),\n",
    "    np.float64(-29.417220879306505),\n",
    "    np.float64(17.091582763174387),\n",
    "    np.float64(47.63993828348096),\n",
    "    np.float64(-0.37098694528112475),\n",
    "    np.float64(16.494296739158074),\n",
    "    np.float64(-15.70809082666571),\n",
    "    np.float64(20.14970284570579),\n",
    "    np.float64(-26.00176547864107),\n",
    "    np.float64(-6.360533139169547),\n",
    "    np.float64(25.68282699840833),\n",
    "    np.float64(-10.15048542291018),\n",
    "    np.float64(2.950293556678176),\n",
    "    np.float64(-11.095398152509041),\n",
    "    np.float64(-13.138006728457995),\n",
    "    np.float64(-48.342934892878844),\n",
    "    np.float64(30.205978814489015),\n",
    "    np.float64(41.365831977060566),\n",
    "    np.float64(-48.55076351786226),\n",
    "    np.float64(13.251929178775242),\n",
    "    np.float64(-29.66243040316211),\n",
    "    np.float64(4.81297022684786),\n",
    "    np.float64(23.999452456264265),\n",
    "    np.float64(-38.07421964388793),\n",
    "    np.float64(40.48141518769326),\n",
    "    np.float64(33.50939440547698),\n",
    "    np.float64(5.94352833775195),\n",
    "    np.float64(-14.917564874677602),\n",
    "    np.float64(29.149490954062344),\n",
    "    np.float64(37.32228652297634),\n",
    "    np.float64(14.967169889941971),\n",
    "    np.float64(-44.05940662730967),\n",
    "    np.float64(7.648411873148042),\n",
    "    np.float64(-44.56059204064932),\n",
    "    np.float64(-40.70317522772816),\n",
    "    np.float64(46.516098470387504),\n",
    "    np.float64(-0.2356735968672936),\n",
    "    np.float64(14.37454035369845),\n",
    "    np.float64(45.27955020251299),\n",
    "    np.float64(28.950058323731525),\n",
    "    np.float64(-41.14962358253518),\n",
    "    np.float64(46.93055773892196),\n",
    "    np.float64(11.41670173618926),\n",
    "    np.float64(34.934077778462424),\n",
    "    np.float64(-48.66036892896053),\n",
    "    np.float64(42.585363922710755),\n",
    "    np.float64(-20.604891138999047),\n",
    "    np.float64(37.17036513260477),\n",
    "    np.float64(-2.7279324216339518),\n",
    "    np.float64(33.72134376095261),\n",
    "    np.float64(-14.56782155587915),\n",
    "    np.float64(13.866445185066425),\n",
    "    np.float64(-33.78078967205901),\n",
    "    np.float64(-6.228692224722408),\n",
    "    np.float64(45.58071650576646),\n",
    "    np.float64(-31.71025387631604),\n",
    "    np.float64(26.88713154456633),\n",
    "    np.float64(-6.5184339295439955),\n",
    "    np.float64(-32.53771476592664),\n",
    "    np.float64(-35.37195638561976),\n",
    "    np.float64(47.262677932156166),\n",
    "    np.float64(-3.47052841360599),\n",
    "    np.float64(-13.927622054517506),\n",
    "    np.float64(-42.55528191830058),\n",
    "    np.float64(-48.680220567197765),\n",
    "    np.float64(45.22061189856021),\n",
    "    np.float64(2.5342565210016303),\n",
    "    np.float64(13.404070069310293),\n",
    "    np.float64(-1.4456787607444497),\n",
    "    np.float64(-23.955968918677495),\n",
    "    np.float64(41.11613282587069),\n",
    "    np.float64(-46.84149495210313),\n",
    "    np.float64(33.040948255243634),\n",
    "    np.float64(-44.28926368951143),\n",
    "    np.float64(42.16800422738848),\n",
    "    np.float64(-48.29459095500661),\n",
    "    np.float64(36.27788715154675),\n",
    "    np.float64(-35.31531256996938),\n",
    "    np.float64(46.50487227558651),\n",
    "    np.float64(-44.98148310879204),\n",
    "    np.float64(-17.63941255556646),\n",
    "    np.float64(6.388262478319675),\n",
    "    np.float64(19.470513405841984),\n",
    "    np.float64(4.529740877315412),\n",
    "    np.float64(6.042298206332887),\n",
    "    np.float64(-20.369743983352684),\n",
    "    np.float64(9.505186596241913),\n",
    "    np.float64(31.51358554712229),\n",
    "    np.float64(19.1847880991687),\n",
    "    np.float64(-31.301110907602737),\n",
    "    np.float64(-19.884318923813648),\n",
    "    np.float64(-5.458622534219991),\n",
    "    np.float64(38.3519546297275),\n",
    "    np.float64(-47.47834907795876),\n",
    "    np.float64(34.68624186853981),\n",
    "    np.float64(-8.509385408953463),\n",
    "    np.float64(-28.035577271309265),\n",
    "    np.float64(-20.46037058234117),\n",
    "    np.float64(30.620184847139527),\n",
    "    np.float64(-31.986938720346693),\n",
    "    np.float64(-28.25031216641306),\n",
    "    np.float64(-38.46926912817102),\n",
    "    np.float64(36.41014036061213),\n",
    "    np.float64(-19.60131440188979),\n",
    "    np.float64(-49.74664974787506),\n",
    "    np.float64(30.362595805841565),\n",
    "    np.float64(30.350254970081508),\n",
    "    np.float64(-42.68519721247904),\n",
    "    np.float64(-38.85429145850138),\n",
    "    np.float64(18.32049726127073),\n",
    "    np.float64(42.99363798146668),\n",
    "    np.float64(-35.74978037341589),\n",
    "    np.float64(0.9736392671204968),\n",
    "    np.float64(17.38053394940995),\n",
    "    np.float64(17.384814293274005),\n",
    "    np.float64(-9.818447283291434),\n",
    "    np.float64(-40.789789668522005),\n",
    "    np.float64(-4.694810379357882),\n",
    "    np.float64(32.44150052750783),\n",
    "    np.float64(44.68088326506468),\n",
    "    np.float64(-20.39501902593226),\n",
    "    np.float64(-38.384014424611145),\n",
    "    np.float64(14.405698071772775),\n",
    "    np.float64(25.107553714392765),\n",
    "    np.float64(-45.61353944289446),\n",
    "    np.float64(-2.61175286870413),\n",
    "    np.float64(-31.58888142088887),\n",
    "    np.float64(-8.788630263721096),\n",
    "    np.float64(-22.5795784037093),\n",
    "    np.float64(-0.547681751830865),\n",
    "    np.float64(-47.87792320441467),\n",
    "    np.float64(-34.044757554341246),\n",
    "    np.float64(-1.9686773450949246),\n",
    "    np.float64(15.020398291102865),\n",
    "    np.float64(32.83301897224325),\n",
    "    np.float64(12.190904651394597),\n",
    "    np.float64(-19.718466125523758),\n",
    "    np.float64(23.264969082578475),\n",
    "    np.float64(-46.454655483388365),\n",
    "    np.float64(1.0042798957039167),\n",
    "    np.float64(-2.5276414729686536),\n",
    "    np.float64(-36.10462101019033),\n",
    "    np.float64(-1.8227802502133201),\n",
    "    np.float64(35.39379292485468),\n",
    "    np.float64(22.736238484689977),\n",
    "    np.float64(-5.578982285019208),\n",
    "    np.float64(-7.850560832131414),\n",
    "    np.float64(-32.59527574415411),\n",
    "    np.float64(-32.000915434107725),\n",
    "    np.float64(33.484986062942056),\n",
    "    np.float64(34.45103095535421),\n",
    "    np.float64(-34.81752630082228),\n",
    "    np.float64(-14.249809801131498),\n",
    "    np.float64(28.069637818536464),\n",
    "    np.float64(17.97899557515295),\n",
    "    np.float64(34.9830961013225),\n",
    "    np.float64(34.319752785363974),\n",
    "    np.float64(-16.77170129278897),\n",
    "    np.float64(-47.98149157577707),\n",
    "    np.float64(-33.00489311508409),\n",
    "    np.float64(-26.949120512665615),\n",
    "    np.float64(49.228560334184635),\n",
    "    np.float64(17.073471887883827),\n",
    "    np.float64(-38.31371725192312),\n",
    "    np.float64(-42.276162034881835),\n",
    "    np.float64(42.166930724100766),\n",
    "    np.float64(-20.96796416248151),\n",
    "    np.float64(-21.671943456238786),\n",
    "    np.float64(40.34769769569178),\n",
    "    np.float64(-11.5462793119828),\n",
    "    np.float64(-49.66969641513291),\n",
    "    np.float64(-47.51010719809173),\n",
    "    np.float64(-22.00367174135467),\n",
    "    np.float64(45.56561884579958),\n",
    "    np.float64(-1.2704214953639195),\n",
    "    np.float64(-24.323965955741443),\n",
    "    np.float64(39.64024477961652),\n",
    "    np.float64(-26.52340103865878),\n",
    "    np.float64(38.55158978236179),\n",
    "    np.float64(-8.199715245843109),\n",
    "    np.float64(-19.38775141231198),\n",
    "    np.float64(31.566682427142183),\n",
    "    np.float64(-20.70988455106999),\n",
    "    np.float64(31.524616655239328),\n",
    "    np.float64(-14.246263642113433),\n",
    "    np.float64(7.316681673461055),\n",
    "    np.float64(14.805244695814835),\n",
    "    np.float64(-23.633048225489205),\n",
    "    np.float64(5.324506561057817),\n",
    "    np.float64(-15.338489902178189),\n",
    "    np.float64(-19.952932066379926),\n",
    "    np.float64(-43.84846483263276),\n",
    "    np.float64(11.025355096183532),\n",
    "    np.float64(-27.677939789338847),\n",
    "    np.float64(-8.134932802682414),\n",
    "    np.float64(-47.82023464769855),\n",
    "    np.float64(9.54799808470164),\n",
    "    np.float64(34.76465751495547),\n",
    "    np.float64(46.905234281397156),\n",
    "    np.float64(5.810393459139007),\n",
    "    np.float64(49.17823243521255),\n",
    "    np.float64(-29.057208383450593),\n",
    "    np.float64(-23.61036127097098),\n",
    "    np.float64(-49.63560639935284),\n",
    "    np.float64(-18.798777460579707),\n",
    "    np.float64(-0.9239844129400794),\n",
    "    np.float64(-8.21231063316479),\n",
    "    np.float64(31.03857048785568),\n",
    "    np.float64(31.337263698911556),\n",
    "    np.float64(33.97997227876634),\n",
    "    np.float64(1.834666271498861),\n",
    "    np.float64(-48.715721319040384),\n",
    "    np.float64(1.462028349356082),\n",
    "    np.float64(0.9750703706327002),\n",
    "    np.float64(34.32130518479052),\n",
    "    np.float64(-20.70745585710133),\n",
    "    np.float64(0.1288944660325484),\n",
    "    np.float64(33.3358626765082),\n",
    "    np.float64(41.223877355703266),\n",
    "    np.float64(25.801603373085953),\n",
    "    np.float64(-40.55288503149369),\n",
    "    np.float64(40.415523626877174),\n",
    "    np.float64(26.15409114406691),\n",
    "    np.float64(-29.086974535406195),\n",
    "    np.float64(-49.33219692514403),\n",
    "    np.float64(22.315209843061652),\n",
    "    np.float64(-38.43315751208469),\n",
    "    np.float64(-39.29627810564437),\n",
    "    np.float64(-29.463471149786336),\n",
    "    np.float64(-19.087934731796096),\n",
    "    np.float64(-0.17816809642715725),\n",
    "    np.float64(-39.48602857069097),\n",
    "    np.float64(-10.648371706404959),\n",
    "    np.float64(-5.112309692582606),\n",
    "    np.float64(31.562986492670603),\n",
    "    np.float64(24.377370841496415),\n",
    "    np.float64(-3.696755026561924),\n",
    "    np.float64(9.219707542246212),\n",
    "    np.float64(-16.046720929419067),\n",
    "    np.float64(6.519495448565124),\n",
    "    np.float64(40.28323862286098),\n",
    "    np.float64(-11.36628521684252),\n",
    "    np.float64(-37.59578845667252),\n",
    "    np.float64(41.35538230605232),\n",
    "    np.float64(-25.466997592743688),\n",
    "    np.float64(34.268686127536014),\n",
    "    np.float64(4.144018621727476),\n",
    "    np.float64(11.172128131060845),\n",
    "    np.float64(-23.98958299764401),\n",
    "    np.float64(1.7051083227167325),\n",
    "    np.float64(-11.122197530207359),\n",
    "    np.float64(-18.220794934750685),\n",
    "    np.float64(15.19503864338732),\n",
    "    np.float64(-24.430563458823407),\n",
    "    np.float64(44.98255255152125),\n",
    "    np.float64(4.055269459343734),\n",
    "    np.float64(-29.254646338148326),\n",
    "    np.float64(-13.100943051077948),\n",
    "    np.float64(-39.56268176991043),\n",
    "    np.float64(18.859099307417864),\n",
    "    np.float64(18.239108516341602),\n",
    "    np.float64(-44.95517656388807),\n",
    "    np.float64(-18.531714866462423),\n",
    "    np.float64(-30.08808390628559),\n",
    "    np.float64(-0.9995047347910457),\n",
    "    np.float64(0.2961357341817461),\n",
    "    np.float64(-47.12406689269651),\n",
    "    np.float64(-6.332397583497226),\n",
    "    np.float64(41.21211987971671),\n",
    "    np.float64(33.11296671846762),\n",
    "    np.float64(25.88919689503686),\n",
    "    np.float64(40.66203023914599),\n",
    "    np.float64(45.52144926065132),\n",
    "    np.float64(35.9518626562848),\n",
    "    np.float64(0.9422045874856195),\n",
    "    np.float64(-37.4642239852246),\n",
    "    np.float64(-38.38978999999427),\n",
    "    np.float64(20.930070386323436),\n",
    "    np.float64(-48.78089527952395),\n",
    "    np.float64(-23.12639505997549),\n",
    "    np.float64(-13.260700817233548),\n",
    "    np.float64(49.9418487264963),\n",
    "    np.float64(1.2086541320112971),\n",
    "    np.float64(16.049527148998166),\n",
    "    np.float64(-38.628631967627335),\n",
    "    np.float64(2.256319571569044),\n",
    "    np.float64(-12.444189447379095),\n",
    "    np.float64(33.542045315113114),\n",
    "    np.float64(21.77719580811703),\n",
    "    np.float64(-47.86533794862451),\n",
    "    np.float64(48.45947232622113),\n",
    "    np.float64(29.308745697961086),\n",
    "    np.float64(8.856895432940156),\n",
    "    np.float64(12.458370349344229),\n",
    "    np.float64(45.17843936099196),\n",
    "    np.float64(-10.85032087004241),\n",
    "    np.float64(-25.085765970749343),\n",
    "    np.float64(49.14812527056711),\n",
    "    np.float64(-35.3520731594133),\n",
    "    np.float64(13.785572128017243),\n",
    "    np.float64(-24.27271986746392),\n",
    "    np.float64(-35.51022290156133),\n",
    "    np.float64(46.966367544653735),\n",
    "    np.float64(48.922681709885524),\n",
    "    np.float64(25.710253977377974),\n",
    "    np.float64(8.913248500906349),\n",
    "    np.float64(1.4135495671127032),\n",
    "    np.float64(49.14654806314451),\n",
    "    np.float64(-47.02990713242343),\n",
    "    np.float64(-41.32118781570499),\n",
    "    np.float64(-40.922860799355064),\n",
    "    np.float64(17.932312944589796),\n",
    "    np.float64(22.941518262999722),\n",
    "    np.float64(-23.731223423106805),\n",
    "    np.float64(-39.376594015132504),\n",
    "    np.float64(10.58674360968159),\n",
    "    np.float64(37.65383642242685),\n",
    "    np.float64(41.405445507311995),\n",
    "    np.float64(-18.204206030150015),\n",
    "    np.float64(-48.86080331653654),\n",
    "    np.float64(-6.38658270030048),\n",
    "    np.float64(-1.4442837072328416),\n",
    "    np.float64(7.305561084468913),\n",
    "    np.float64(12.742858685514356),\n",
    "    np.float64(-34.26283267011242),\n",
    "    np.float64(-13.433578919413002),\n",
    "    np.float64(46.711639745805684),\n",
    "    np.float64(-27.10761624180882),\n",
    "    np.float64(37.032502485157906),\n",
    "    np.float64(23.66661374818507),\n",
    "    np.float64(32.85724661957771),\n",
    "    np.float64(49.01722404962858),\n",
    "    np.float64(0.8167331911306377),\n",
    "    np.float64(-37.43939595982015),\n",
    "    np.float64(-45.71092459293882),\n",
    "    np.float64(-31.218268601047672),\n",
    "    np.float64(31.09131098848485),\n",
    "    np.float64(49.52890022287471),\n",
    "    np.float64(3.5301591824211584),\n",
    "    np.float64(15.939901659209326),\n",
    "    np.float64(40.07422582906827),\n",
    "    np.float64(-11.148904180641885),\n",
    "    np.float64(36.30820774420317),\n",
    "    np.float64(-34.424640854966704),\n",
    "    np.float64(-30.217144591174115),\n",
    "    np.float64(27.914950163786315),\n",
    "    np.float64(-18.950738099436805),\n",
    "    np.float64(-11.844171837443774),\n",
    "    np.float64(-45.438462103106545),\n",
    "    np.float64(-39.547141517252925),\n",
    "    np.float64(-35.11061901308212),\n",
    "    np.float64(14.575843693238625),\n",
    "    np.float64(16.466766857760746),\n",
    "    np.float64(23.355645976718506),\n",
    "    np.float64(-40.071019739871026),\n",
    "    np.float64(32.06896981229448),\n",
    "    np.float64(-32.75019583465351),\n",
    "    np.float64(12.54434524428315),\n",
    "    np.float64(-7.667630624462106),\n",
    "    np.float64(47.893865472623304),\n",
    "    np.float64(42.980934345750526),\n",
    "    np.float64(-3.34687714102769),\n",
    "    np.float64(5.491028545316524),\n",
    "    np.float64(28.584616371424175),\n",
    "    np.float64(-32.41708788461827),\n",
    "    np.float64(-28.96946980814731),\n",
    "    np.float64(-10.804876783519),\n",
    "    np.float64(-48.75256288529339),\n",
    "    np.float64(-12.201689464019026),\n",
    "    np.float64(-35.31726331605933),\n",
    "    np.float64(17.570515200017965),\n",
    "    np.float64(-33.442928852506256),\n",
    "    np.float64(14.364041236383855),\n",
    "    np.float64(15.77447723813583),\n",
    "    np.float64(-1.4546028835129547),\n",
    "    np.float64(-28.8684831920335),\n",
    "    np.float64(-14.516809159510402),\n",
    "    np.float64(-4.464907785659513),\n",
    "    np.float64(10.02166273075997),\n",
    "    np.float64(32.790495358648215),\n",
    "    np.float64(39.62250409295417),\n",
    "    np.float64(-30.719024629750567),\n",
    "    np.float64(-29.67291890315161),\n",
    "    np.float64(-24.60988360212799),\n",
    "    np.float64(-38.2057800598944),\n",
    "    np.float64(35.923224754592695),\n",
    "    np.float64(5.206359267483585),\n",
    "    np.float64(39.11953587700164),\n",
    "    np.float64(12.26655243684246),\n",
    "    np.float64(-27.29934645501695),\n",
    "    np.float64(33.76424755119619),\n",
    "    np.float64(-10.92063184008294),\n",
    "    np.float64(-14.427219911348033),\n",
    "    np.float64(27.707563607400417),\n",
    "    np.float64(30.663537122033844),\n",
    "    np.float64(-45.699599258204735),\n",
    "    np.float64(-39.063036536981585),\n",
    "    np.float64(-16.377720657742323),\n",
    "    np.float64(40.32021079670103),\n",
    "    np.float64(47.10366281371449),\n",
    "    np.float64(16.388107684852898),\n",
    "    np.float64(-49.06583398863017),\n",
    "    np.float64(44.241991247419904),\n",
    "    np.float64(8.96474145041907),\n",
    "    np.float64(-39.18167981961348),\n",
    "    np.float64(-26.25819868673176),\n",
    "    np.float64(20.514501691795857),\n",
    "    np.float64(-41.013873370032094),\n",
    "    np.float64(-19.63456828493496),\n",
    "    np.float64(34.70930395848242),\n",
    "    np.float64(0.41897697300716885),\n",
    "    np.float64(-33.44298461768052),\n",
    "    np.float64(-21.89704079620253),\n",
    "    np.float64(-11.872919449265993),\n",
    "    np.float64(-41.09016144376545),\n",
    "    np.float64(-47.97060554201052),\n",
    "    np.float64(-24.29266454741571),\n",
    "    np.float64(-23.63060428107593),\n",
    "    np.float64(21.03203566243053),\n",
    "    np.float64(-5.936038486442619),\n",
    "    np.float64(-49.45036613013216),\n",
    "    np.float64(14.978781289477169),\n",
    "    np.float64(28.54756300444504),\n",
    "    np.float64(-4.472010153843087),\n",
    "    np.float64(-9.063674105360285),\n",
    "    np.float64(-17.07682093602549),\n",
    "    np.float64(-44.6354007064557),\n",
    "    np.float64(-15.091356865090042),\n",
    "    np.float64(47.89130003521197),\n",
    "    np.float64(19.207741222653624),\n",
    "    np.float64(-36.05814271386105),\n",
    "    np.float64(31.969427435126292),\n",
    "    np.float64(-10.553094220157654),\n",
    "    np.float64(7.636693664142058),\n",
    "    np.float64(-5.497914134032264),\n",
    "    np.float64(36.075089126961885),\n",
    "    np.float64(-47.379741081773915),\n",
    "    np.float64(-7.379722601656681),\n",
    "    np.float64(-29.937550164516768),\n",
    "    np.float64(35.76003650144692),\n",
    "    np.float64(25.666316373610954),\n",
    "    np.float64(5.6636404226318575),\n",
    "    np.float64(1.7940703443473112),\n",
    "    np.float64(-22.83913051930352),\n",
    "    np.float64(-49.206548206124836),\n",
    "    np.float64(-13.243752940134371),\n",
    "    np.float64(4.8668177364169765),\n",
    "    np.float64(21.237487233816736),\n",
    "    np.float64(-33.09857818383826),\n",
    "    np.float64(38.355631829047),\n",
    "    np.float64(-44.07599395213152),\n",
    "    np.float64(-26.780204732959156),\n",
    "    np.float64(-40.539874483929104),\n",
    "    np.float64(32.93597366446856),\n",
    "    np.float64(7.359727646493887),\n",
    "    np.float64(44.63117587349329),\n",
    "    np.float64(-5.948786451547207),\n",
    "    np.float64(7.11421148128327),\n",
    "    np.float64(45.080141421702336),\n",
    "    np.float64(-23.08489783978852),\n",
    "    np.float64(-29.481245895872952),\n",
    "    np.float64(-33.872013722635515),\n",
    "    np.float64(10.04096774307942),\n",
    "    np.float64(25.32096695955954),\n",
    "    np.float64(33.45856895113775),\n",
    "    np.float64(28.977539140321568),\n",
    "    np.float64(-6.619603639257697),\n",
    "    np.float64(34.11838736968832),\n",
    "    np.float64(-11.30849821321658),\n",
    "    np.float64(26.931392299425738),\n",
    "    np.float64(13.368684769810912),\n",
    "    np.float64(-15.195530888783892),\n",
    "    np.float64(-49.75487488830317),\n",
    "    np.float64(-9.892697574172196),\n",
    "    np.float64(-28.66395549913897),\n",
    "    np.float64(-19.92537550677622),\n",
    "    np.float64(-12.261824972415347),\n",
    "    np.float64(-18.827754632077976),\n",
    "    np.float64(40.828862825113504),\n",
    "    np.float64(-49.46400286202625),\n",
    "    np.float64(-44.77077180041212),\n",
    "    np.float64(-16.505314148448313),\n",
    "    np.float64(-44.13328097485631),\n",
    "    np.float64(5.487771850241039),\n",
    "    np.float64(-21.331812566943885),\n",
    "    np.float64(20.573329528034705),\n",
    "    np.float64(-27.18397810815333),\n",
    "    np.float64(14.023678306307431),\n",
    "    np.float64(29.020890393771154),\n",
    "    np.float64(39.717287258539045),\n",
    "    np.float64(-44.33652448584529),\n",
    "    np.float64(15.54001792731981),\n",
    "    np.float64(-47.675573022665915),\n",
    "    np.float64(-47.82465286576107),\n",
    "    np.float64(-44.3699415156555),\n",
    "    np.float64(17.26671405961018),\n",
    "    np.float64(19.64954284591606),\n",
    "    np.float64(-14.347143518275708),\n",
    "    np.float64(43.061847490214504),\n",
    "    np.float64(20.24898960937324),\n",
    "    np.float64(40.97436120513842),\n",
    "    np.float64(8.145882136556828),\n",
    "    np.float64(-22.498806991846145),\n",
    "    np.float64(-29.138292351179274),\n",
    "    np.float64(16.9456413777273),\n",
    "    np.float64(11.958864438901927),\n",
    "    np.float64(-16.370611140476385),\n",
    "    np.float64(-47.459728232081474),\n",
    "    np.float64(-48.36087609428934),\n",
    "    np.float64(37.94494293650314),\n",
    "    np.float64(-13.179125137127414),\n",
    "    np.float64(40.41173737088826),\n",
    "    np.float64(-49.47665900106904),\n",
    "    np.float64(-11.361546851358995),\n",
    "    np.float64(-3.328568762160444),\n",
    "    np.float64(25.55860330454786),\n",
    "    np.float64(22.53008037918046),\n",
    "    np.float64(-27.52455504628477),\n",
    "    np.float64(1.6638633913500769),\n",
    "    np.float64(26.99721953575869),\n",
    "    np.float64(37.810029662335026),\n",
    "    np.float64(-8.34379453137376),\n",
    "    np.float64(28.778559253885533),\n",
    "    np.float64(-10.093979654469209),\n",
    "    np.float64(-1.0785811994417713),\n",
    "    np.float64(29.8426632467814),\n",
    "    np.float64(28.610517572202014),\n",
    "    np.float64(42.8094067082732),\n",
    "    np.float64(-12.188526847495005),\n",
    "    np.float64(30.19843172062916),\n",
    "    np.float64(14.577918126812804),\n",
    "    np.float64(26.388824570891032),\n",
    "    np.float64(-16.983312668564253),\n",
    "    np.float64(-27.071592544521017),\n",
    "    np.float64(6.070434674592583),\n",
    "    np.float64(-1.635397721087351),\n",
    "    np.float64(0.6760135981035091),\n",
    "    np.float64(1.4887326020862872),\n",
    "    np.float64(-5.48204674116711),\n",
    "    np.float64(-1.8166364222240148),\n",
    "    np.float64(8.58164392959938),\n",
    "    np.float64(38.57279885481721),\n",
    "    np.float64(-39.91007110001085),\n",
    "    np.float64(-1.9808823997032547),\n",
    "    np.float64(41.700560569956124),\n",
    "    np.float64(43.93292523631598),\n",
    "    np.float64(-44.813506675841374),\n",
    "    np.float64(28.580238851585946),\n",
    "    np.float64(23.93627291935772),\n",
    "    np.float64(32.71794435985966),\n",
    "    np.float64(10.016257755574998),\n",
    "    np.float64(-14.156737328185521),\n",
    "    np.float64(45.89309397943626),\n",
    "    np.float64(-44.71241775247306),\n",
    "    np.float64(18.2260337512882),\n",
    "    np.float64(17.242651628619456),\n",
    "    np.float64(-18.586221969871463),\n",
    "    np.float64(-25.714645961159345),\n",
    "    np.float64(43.02488254806997),\n",
    "    np.float64(-25.88378640826895),\n",
    "    np.float64(-17.247911068060276),\n",
    "    np.float64(31.991399092303936),\n",
    "    np.float64(3.104305833759369),\n",
    "    np.float64(-19.57260701993887),\n",
    "    np.float64(5.890571127812571),\n",
    "    np.float64(13.026308187218646),\n",
    "    np.float64(-20.167471478868492),\n",
    "    np.float64(-44.95285845587376),\n",
    "    np.float64(36.12673785134092),\n",
    "    np.float64(5.480679283207145),\n",
    "    np.float64(3.714084856474841),\n",
    "    np.float64(-5.578526316121554),\n",
    "    np.float64(-2.8461151961491566),\n",
    "    np.float64(39.90974602370004),\n",
    "    np.float64(-15.965966105870201),\n",
    "    np.float64(-39.73900108796546),\n",
    "    np.float64(0.6474079307623128),\n",
    "    np.float64(45.29789789206575),\n",
    "    np.float64(-24.928458922197382),\n",
    "    np.float64(-26.849643900217536),\n",
    "    np.float64(44.53556380579634),\n",
    "    np.float64(-7.31609702450352),\n",
    "    np.float64(16.270967740591985),\n",
    "    np.float64(33.52846368187478),\n",
    "    np.float64(40.213281260218864),\n",
    "    np.float64(-30.294489951364945),\n",
    "    np.float64(-4.379039941946218),\n",
    "    np.float64(21.32220377941647),\n",
    "    np.float64(-49.38505020162255),\n",
    "    np.float64(48.50640399492781),\n",
    "    np.float64(-39.43151968224473),\n",
    "    np.float64(1.939985630779141),\n",
    "    np.float64(17.2292888310417),\n",
    "    np.float64(29.082203318751624),\n",
    "    np.float64(48.49788382709539),\n",
    "    np.float64(-10.050860145003547),\n",
    "    np.float64(-49.96013174105421),\n",
    "    np.float64(32.84186315812528),\n",
    "    np.float64(38.840734413858854),\n",
    "    np.float64(11.221860856270105),\n",
    "    np.float64(3.2169240433579134),\n",
    "    np.float64(25.973922727342995),\n",
    "    np.float64(3.610646024259246),\n",
    "    np.float64(25.750060437327335),\n",
    "    np.float64(30.80755655539643),\n",
    "    np.float64(-9.691155566946776),\n",
    "    np.float64(-14.223236885090309),\n",
    "    np.float64(-47.79120911150575),\n",
    "    np.float64(-25.53992697216919),\n",
    "    np.float64(-45.159222082283044),\n",
    "    np.float64(26.66301883183),\n",
    "    np.float64(23.911330511803712),\n",
    "    np.float64(-31.24951354692158),\n",
    "    np.float64(45.945674263090325),\n",
    "    np.float64(-13.023037707867736),\n",
    "    np.float64(1.7818946754943639),\n",
    "    np.float64(26.983252483499427)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "428b16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shaped_params = shape_model_params(params = params, params_shape = [(81,8),(8,),(8,1),(1,)])\n",
    "model.set_weights(shaped_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "060ed185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 150487248.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150487248.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d13d0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_Valence</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>3.393948</td>\n",
       "      <td>0.651105</td>\n",
       "      <td>-0.576274</td>\n",
       "      <td>0.34169</td>\n",
       "      <td>-0.649957</td>\n",
       "      <td>2.242095</td>\n",
       "      <td>1.898454</td>\n",
       "      <td>1.416547</td>\n",
       "      <td>-0.708958</td>\n",
       "      <td>1.190562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.508873</td>\n",
       "      <td>-0.87697</td>\n",
       "      <td>-0.498686</td>\n",
       "      <td>-0.826351</td>\n",
       "      <td>2.152587</td>\n",
       "      <td>1.223181</td>\n",
       "      <td>0.771937</td>\n",
       "      <td>-0.417513</td>\n",
       "      <td>0.213481</td>\n",
       "      <td>-0.649801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "19619            3.393948          0.651105             -0.576274   \n",
       "\n",
       "       gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "19619            0.34169              -0.649957             2.242095   \n",
       "\n",
       "       wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "19619                 1.898454           1.416547              -0.708958   \n",
       "\n",
       "       std_atomic_mass  ...  mean_Valence  wtd_mean_Valence  gmean_Valence  \\\n",
       "19619         1.190562  ...     -0.508873          -0.87697      -0.498686   \n",
       "\n",
       "       wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\n",
       "19619          -0.826351         2.152587             1.223181       0.771937   \n",
       "\n",
       "       wtd_range_Valence  std_Valence  wtd_std_Valence  \n",
       "19619          -0.417513     0.213481        -0.649801  \n",
       "\n",
       "[1 rows x 81 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5a16831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       critical_temp\n",
       "19619           85.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e061430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1390.3143],\n",
       "       [ 3289.9207],\n",
       "       [10575.874 ],\n",
       "       ...,\n",
       "       [ 1238.2097],\n",
       "       [ 2914.7153],\n",
       "       [ 2216.1787]], shape=(5316, 1), dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a3fcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function # OPTIONAL: This decorator compiles the function to XLA/Graph (makes it even faster)\n",
    "def train_step(\n",
    "    model: tf.keras.Model, \n",
    "    X_tensor: tf.Tensor, \n",
    "    y_tensor: tf.Tensor, \n",
    "    learning_rate: float\n",
    ") -> float:\n",
    "    # Defensive Casting: Ensure inputs are float32 no matter what was passed in\n",
    "    X_tensor = tf.cast(X_tensor, dtype=tf.float32)\n",
    "    y_tensor = tf.cast(y_tensor, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward Pass\n",
    "        predictions = model(X_tensor, training=True) \n",
    "\n",
    "        # Fix Shapes\n",
    "        predictions = tf.reshape(predictions, [-1])\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = tf.math.reduce_mean(tf.math.square(predictions - y_tensor))\n",
    "\n",
    "    # Backward Pass\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Manual Update\n",
    "    for param, grad in zip(model.trainable_variables, gradients):\n",
    "        if grad is not None:\n",
    "            param.assign_sub(grad * learning_rate)\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e61d6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    import tensorflow as tf\n",
    "    for layer in model.layers:\n",
    "        # Reset Kernel (Weights)\n",
    "        if hasattr(layer, 'kernel_initializer') and hasattr(layer, 'kernel'):\n",
    "            layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n",
    "        \n",
    "        # Reset Bias\n",
    "        if hasattr(layer, 'bias_initializer') and hasattr(layer, 'bias'):\n",
    "            layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n",
    "            \n",
    "        # Reset other variables (like LSTM states if you had them)\n",
    "        if hasattr(layer, 'recurrent_initializer') and hasattr(layer, 'recurrent_kernel'):\n",
    "            layer.recurrent_kernel.assign(layer.recurrent_initializer(tf.shape(layer.recurrent_kernel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "458aa4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=32,\n",
    "    warm_start: bool = True\n",
    "):\n",
    "    import math\n",
    "\n",
    "    if not warm_start:\n",
    "        print(\"Cold Start: Resetting model weights to random...\")\n",
    "        reset_weights(model)\n",
    "    else:\n",
    "        print(\"Warm Start: Continuing training with existing weights...\")\n",
    "\n",
    "    df_train = X_train.join(y_train)\n",
    "    num_samples = len(df_train)\n",
    "    total_batches = math.ceil(num_samples / batch_size)\n",
    "    best_loss: int = None\n",
    "    best_params: list = None\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(f\"--- Starting Epoch {i+1}/{epochs} ---\")\n",
    "        shuffled_df = df_train.sample(frac=1)\n",
    "        X_shuffled = shuffled_df.iloc[:,:-1]\n",
    "        y_shuffled = shuffled_df.iloc[:,-1]\n",
    "\n",
    "        for j in range(0, num_samples, batch_size):\n",
    "            current_batch = (j // batch_size) + 1\n",
    "            \n",
    "            # 1. Get the batch as Pandas objects\n",
    "            X_batch_df = X_shuffled.iloc[j : j + batch_size]\n",
    "            y_batch_series = y_shuffled.iloc[j : j + batch_size]\n",
    "\n",
    "            # 2. Convert batch to tensors\n",
    "            X_tensor = tf.convert_to_tensor(X_batch_df.values, dtype=tf.float32)\n",
    "            y_tensor = tf.convert_to_tensor(y_batch_series.values, dtype=tf.float32)\n",
    "\n",
    "            # 3. Pass Tensors to the function\n",
    "            loss = train_step(model, X_tensor, y_tensor, learning_rate)\n",
    "            \n",
    "            # (Convert loss back to a regular number for printing\n",
    "            print(f\"  Batch {current_batch}/{total_batches} - Loss: {loss.numpy():.4f}\", end=\"\\r\", flush=True)\n",
    "        \n",
    "            if best_loss == None:\n",
    "                best_loss = loss\n",
    "                best_params = model.get_weights()\n",
    "            elif loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = model.get_weights()\n",
    "\n",
    "        print() \n",
    "\n",
    "    print(\"Training Complete.\")\n",
    "    print()\n",
    "    print(f\"best loss {best_loss}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c75589a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 499/499 - Loss: 643.58032\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 217.90637\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 461.6154\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 443.29857\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 269.9876\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 394.3110\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 397.7220\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 356.2090\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 283.3135\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 164.9687\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 175.3266\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 108.8956\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 213.7181\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 1780.7677\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 278.4311\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 127.4014\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 228.1470\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 423.3703\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 170.2615\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 277.4922\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 46.39054\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 194.2417\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 184.1074\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 414.0178\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 273.0599\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 176.3105\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 559.1425\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 363.1254\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 151.1109\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 486.1180\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 242.6516\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 271.7994\n",
      "Training Complete.\n",
      "\n",
      "best loss 24.529308319091797\n"
     ]
    }
   ],
   "source": [
    "params = train(\n",
    "    model = model,\n",
    "    X_train = X_train,\n",
    "    y_train = y_train,\n",
    "    epochs = 32,\n",
    "    learning_rate = 0.001,\n",
    "    warm_start = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c3adbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 226.0166\n",
      "Final RMSE: 15.033847513026128\n"
     ]
    }
   ],
   "source": [
    "# 1. Reshape y_test to be 2D (N rows, 1 column)\n",
    "# .values turns it into numpy, .reshape(-1, 1) gives it the column shape\n",
    "y_test_reshaped = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# 2. Compile the model (Required for evaluate!)\n",
    "# You must tell Keras what loss to use, since it doesn't know about your manual calculation\n",
    "model.compile(loss='mean_squared_error', optimizer='adam') \n",
    "\n",
    "# 3. Evaluate using the reshaped data\n",
    "loss = model.evaluate(X_test, y_test_reshaped)\n",
    "print(\"Final RMSE:\", loss**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c81dbefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gaussian_dropout                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianDropout</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gaussian_dropout                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGaussianDropout\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,945</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,945\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,945</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,945\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GaussianDropout\n",
    "\n",
    "# New model\n",
    "model = Sequential()\n",
    "\n",
    "num_cols = X.shape[1]\n",
    "\n",
    "# add hidden layers\n",
    "model.add(Dense(8, input_shape = (num_cols,), activation = 'relu'))\n",
    "model.add(Dense(16, input_shape = (8,), activation = 'relu'))\n",
    "model.add(Dense(32, input_shape = (16,), activation = 'relu'))\n",
    "model.add(Dropout(rate = 0.5))\n",
    "model.add(Dense(32, input_shape = (32,), activation = 'relu'))\n",
    "model.add(GaussianDropout(rate = 0.5))\n",
    "model.add(Dense(16, input_shape = (32,), activation = 'relu'))\n",
    "\n",
    "# add output layer\n",
    "model.add(Dense(1, input_shape = (16,), activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb727353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm Start: Continuing training with existing weights...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 499/499 - Loss: 226.85091\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 419.42688\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 526.90554\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 100.33618\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 466.30208\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 704.78339\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 255.53721\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 862.032876\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 271.04551\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 998.94045\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 363.74794\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 469.89105\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 734.61457\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 861.67528\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 882.79145\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 527.95440\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 881.87957\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 1009.0014\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 1016.6108\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 964.44471\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 589.46688\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 519.50998\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 514.25077\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 504.78715\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 290.95853\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 436.28203\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 346.68585\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 597.15552\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 792.76020\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 468.81724\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 810.97304\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1085.2640\n",
      "Training Complete.\n",
      "\n",
      "best loss 93.28970336914062\n"
     ]
    }
   ],
   "source": [
    "params = train(\n",
    "    model = model,\n",
    "    X_train = X_train,\n",
    "    y_train = y_train,\n",
    "    epochs = 32,\n",
    "    learning_rate = 0.0005,\n",
    "    warm_start = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "727b78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "981ccb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_rmse = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a36fc651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 329.7839\n",
      "Final RMSE: 18.159954172488295\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam') \n",
    "\n",
    "# Evaluate using the reshaped data\n",
    "loss = model.evaluate(X_test, y_test_reshaped)\n",
    "print(\"Final RMSE:\", loss**0.5)\n",
    "if loss**0.5<best_rmse:\n",
    "    best_params = params.copy()\n",
    "    best_rmse = loss**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e00ad1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.421219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34.254362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       critical_temp\n",
       "count   21263.000000\n",
       "mean       34.421219\n",
       "std        34.254362\n",
       "min         0.000210\n",
       "25%         5.365000\n",
       "50%        20.000000\n",
       "75%        63.000000\n",
       "max       185.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4f0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
