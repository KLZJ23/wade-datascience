{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc18d91",
   "metadata": {},
   "source": [
    "# function to find the best model by making and testing random models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e625c6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0                   4         88.944468             57.862692   \n",
       "1                   5         92.729214             58.518416   \n",
       "2                   4         88.944468             57.885242   \n",
       "3                   4         88.944468             57.873967   \n",
       "4                   4         88.944468             57.840143   \n",
       "\n",
       "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0          66.361592              36.116612             1.181795   \n",
       "1          73.132787              36.396602             1.449309   \n",
       "2          66.361592              36.122509             1.181795   \n",
       "3          66.361592              36.119560             1.181795   \n",
       "4          66.361592              36.110716             1.181795   \n",
       "\n",
       "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                 1.062396          122.90607              31.794921   \n",
       "1                 1.057755          122.90607              36.161939   \n",
       "2                 0.975980          122.90607              35.741099   \n",
       "3                 1.022291          122.90607              33.768010   \n",
       "4                 1.129224          122.90607              27.848743   \n",
       "\n",
       "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
       "0        51.968828  ...          2.257143       2.213364           2.219783   \n",
       "1        47.094633  ...          2.257143       1.888175           2.210679   \n",
       "2        51.968828  ...          2.271429       2.213364           2.232679   \n",
       "3        51.968828  ...          2.264286       2.213364           2.226222   \n",
       "4        51.968828  ...          2.242857       2.213364           2.206963   \n",
       "\n",
       "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
       "0         1.368922             1.066221              1           1.085714   \n",
       "1         1.557113             1.047221              2           1.128571   \n",
       "2         1.368922             1.029175              1           1.114286   \n",
       "3         1.368922             1.048834              1           1.100000   \n",
       "4         1.368922             1.096052              1           1.057143   \n",
       "\n",
       "   std_Valence  wtd_std_Valence  critical_temp  \n",
       "0     0.433013         0.437059           29.0  \n",
       "1     0.632456         0.468606           26.0  \n",
       "2     0.433013         0.444697           19.0  \n",
       "3     0.433013         0.440952           22.0  \n",
       "4     0.433013         0.428809           23.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "superconductivty_data = fetch_ucirepo(id=464)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = superconductivty_data.data.features\n",
    "y = superconductivty_data.data.targets\n",
    "\n",
    "df = X.join(y)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b8cf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from previous file\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function # This decorator compiles the function to XLA/Graph (makes it even faster)\n",
    "def train_step(\n",
    "    model: tf.keras.Model, \n",
    "    X_tensor: tf.Tensor, \n",
    "    y_tensor: tf.Tensor, \n",
    "    learning_rate: float\n",
    ") -> float:\n",
    "    # Defensive Casting: Ensure inputs are float32 no matter what was passed in\n",
    "    X_tensor = tf.cast(X_tensor, dtype=tf.float32)\n",
    "    y_tensor = tf.cast(y_tensor, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward Pass\n",
    "        predictions = model(X_tensor, training=True) \n",
    "\n",
    "        # Fix Shapes\n",
    "        predictions = tf.reshape(predictions, [-1])\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = tf.math.reduce_mean(tf.math.square(predictions - y_tensor))\n",
    "\n",
    "    # Backward Pass\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Manual Update\n",
    "    for param, grad in zip(model.trainable_variables, gradients):\n",
    "        if grad is not None:\n",
    "            param.assign_sub(grad * learning_rate)\n",
    "            \n",
    "    return loss\n",
    "\n",
    "def reset_weights(model):\n",
    "    import tensorflow as tf\n",
    "    for layer in model.layers:\n",
    "        # Reset Kernel (Weights)\n",
    "        if hasattr(layer, 'kernel_initializer') and hasattr(layer, 'kernel'):\n",
    "            layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n",
    "        \n",
    "        # Reset Bias\n",
    "        if hasattr(layer, 'bias_initializer') and hasattr(layer, 'bias'):\n",
    "            layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n",
    "            \n",
    "        # Reset other variables (like LSTM states if you had them)\n",
    "        if hasattr(layer, 'recurrent_initializer') and hasattr(layer, 'recurrent_kernel'):\n",
    "            layer.recurrent_kernel.assign(layer.recurrent_initializer(tf.shape(layer.recurrent_kernel)))\n",
    "\n",
    "def train(model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=32,\n",
    "    warm_start: bool = True\n",
    "):\n",
    "    import math\n",
    "\n",
    "    if not warm_start:\n",
    "        print(\"Cold Start: Resetting model weights to random...\")\n",
    "        reset_weights(model)\n",
    "    else:\n",
    "        print(\"Warm Start: Continuing training with existing weights...\")\n",
    "\n",
    "    df_train = X_train.join(y_train)\n",
    "    num_samples = len(df_train)\n",
    "    total_batches = math.ceil(num_samples / batch_size)\n",
    "    best_loss: int = None\n",
    "    best_params: list = None\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(f\"--- Starting Epoch {i+1}/{epochs} ---\")\n",
    "        shuffled_df = df_train.sample(frac=1)\n",
    "        X_shuffled = shuffled_df.iloc[:,:-1]\n",
    "        y_shuffled = shuffled_df.iloc[:,-1]\n",
    "\n",
    "        for j in range(0, num_samples, batch_size):\n",
    "            current_batch = (j // batch_size) + 1\n",
    "            \n",
    "            # 1. Get the batch as Pandas objects\n",
    "            X_batch_df = X_shuffled.iloc[j : j + batch_size]\n",
    "            y_batch_series = y_shuffled.iloc[j : j + batch_size]\n",
    "\n",
    "            # 2. Convert batch to tensors\n",
    "            X_tensor = tf.convert_to_tensor(X_batch_df.values, dtype=tf.float32)\n",
    "            y_tensor = tf.convert_to_tensor(y_batch_series.values, dtype=tf.float32)\n",
    "\n",
    "            # 3. Pass Tensors to the function\n",
    "            loss = train_step(model, X_tensor, y_tensor, learning_rate)\n",
    "            \n",
    "            # (Convert loss back to a regular number for printing\n",
    "            print(f\"  Batch {current_batch}/{total_batches} - Loss: {loss.numpy():.4f}\", end=\"\\r\", flush=True)\n",
    "        \n",
    "            if best_loss == None:\n",
    "                best_loss = loss\n",
    "                best_params = model.get_weights()\n",
    "            elif loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = model.get_weights()\n",
    "            \n",
    "            if loss == float('nan'):\n",
    "                print()\n",
    "                print(\"Training resulted in overflow of loss\")\n",
    "                print()\n",
    "                print(f\"best loss {best_loss}\")\n",
    "                return best_params\n",
    "\n",
    "        print() \n",
    "\n",
    "    print(\"Training Complete.\")\n",
    "    print()\n",
    "    print(f\"best loss {best_loss}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0268d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split the SCALED data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a03e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GaussianDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7f918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "class ModelArchitecture:\n",
    "    def __init__(self, layers_list: list[dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize with a list of dictionaries. Each dict represents a layer.\n",
    "        Example:\n",
    "        [\n",
    "            {'type': 'dense', 'units': 64, 'activation': 'relu'},\n",
    "            {'type': 'dropout', 'rate': 0.5},\n",
    "            {'type': 'dense', 'units': 1, 'activation': 'linear'}\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # Define the expected columns\n",
    "        self.expected_cols = ['type', 'units', 'activation', 'rate']\n",
    "        \n",
    "        if layers_list:\n",
    "            self.df = pd.DataFrame(layers_list)\n",
    "            # Ensure all columns exist (fill missing with None/NaN)\n",
    "            for col in self.expected_cols:\n",
    "                if col not in self.df.columns:\n",
    "                    self.df[col] = None\n",
    "        else:\n",
    "            # Create empty DF with correct columns\n",
    "            self.df = pd.DataFrame(columns=self.expected_cols)\n",
    "\n",
    "    def add_layer(self, layer_type, units=None, activation=None, rate=None):\n",
    "        \"\"\"Helper to add a row to the dataframe\"\"\"\n",
    "        new_row = {\n",
    "            'type': layer_type,\n",
    "            'units': units,\n",
    "            'activation': activation,\n",
    "            'rate': rate\n",
    "        }\n",
    "        # Append logic (concat is preferred over append in new pandas)\n",
    "        self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    def get_layer_info(self, index):\n",
    "        return self.df.iloc[index]\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Iterates through the DataFrame and constructs the Keras model.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        for i, row in self.df.iterrows():\n",
    "            layer_type = row['type']\n",
    "            \n",
    "            # Handle First Layer (needs input_shape)\n",
    "            kwargs = {}\n",
    "            if i == 0:\n",
    "                kwargs['input_shape'] = input_shape\n",
    "\n",
    "            # Add Layer\n",
    "            match layer_type:\n",
    "                case 'dense':\n",
    "                    # int() conversion is needed because pandas usually stores numbers as floats\n",
    "                    units = int(row['units']) \n",
    "                    act = row['activation']\n",
    "                    model.add(Dense(units, activation=act, **kwargs))\n",
    "\n",
    "                case 'dropout':\n",
    "                    rate = row['rate']\n",
    "                    model.add(Dropout(rate=rate))\n",
    "\n",
    "                case 'gaussiandropout':\n",
    "                    rate = row['rate']\n",
    "                    model.add(GaussianDropout(rate=rate))\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def set_random_arch(\n",
    "        self,\n",
    "        num_layers,\n",
    "        layer_types = ['dense'],\n",
    "        dropout_layer_types = ['dropout'],\n",
    "        dropout_chance = 0.5,\n",
    "        min_dropout_rate = 0.1,\n",
    "        max_dropout_rate = 0.5,\n",
    "        min_units = 8,\n",
    "        max_units = 64,\n",
    "        activation = 'relu'\n",
    "    ):\n",
    "        self.__init__()\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(num_layers - 1):\n",
    "            self.add_layer(\n",
    "                layer_type = random.choice(layer_types),\n",
    "                units = random.randint(min_units, max_units),\n",
    "                activation =  activation\n",
    "            )\n",
    "            # Randomly add dropout after each layer\n",
    "            if random.random() < dropout_chance:\n",
    "                self.add_layer(\n",
    "                    layer_type = random.choice(dropout_layer_types),\n",
    "                    rate = random.uniform(min_dropout_rate, max_dropout_rate)\n",
    "                )\n",
    "        \n",
    "        # Add output layer\n",
    "        self.add_layer(\n",
    "            layer_type = 'dense',\n",
    "            units = 1, \n",
    "            activation = 'linear'\n",
    "        )\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b48c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_candidate(\n",
    "    X,\n",
    "    y,\n",
    "    test_number,\n",
    "    num_layers,\n",
    "    layer_types = ['dense'],\n",
    "    dropout_layer_types = ['dropout'],\n",
    "    dropout_chance = 0.5,\n",
    "    min_dropout_rate = 0.1,\n",
    "    max_dropout_rate = 0.5,\n",
    "    min_units = 8,\n",
    "    max_units = 64,\n",
    "    activation = 'relu',\n",
    "    epochs = 16\n",
    "):\n",
    "    \n",
    "    architecture: ModelArchitecture = ModelArchitecture().set_random_arch(\n",
    "        num_layers = num_layers,\n",
    "        layer_types = layer_types,\n",
    "        dropout_layer_types=dropout_layer_types,\n",
    "        dropout_chance = dropout_chance,\n",
    "        min_dropout_rate = min_dropout_rate,\n",
    "        max_dropout_rate = max_dropout_rate,\n",
    "        min_units = min_units,\n",
    "        max_units = max_units,\n",
    "        activation = activation\n",
    "    )\n",
    "    print(f\"{num_layers} layers, attempt #{test_number+1}\")\n",
    "\n",
    "    # Prepare data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)\n",
    "    \n",
    "    # Reshape Y for evaluation\n",
    "    y_test_reshaped = y_test.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    input_dim = X_train.shape[1] \n",
    "    model = architecture.build_model(input_shape=(input_dim,))\n",
    "\n",
    "    train(\n",
    "        model=model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        epochs=32,\n",
    "        learning_rate=0.0005,\n",
    "        warm_start=False\n",
    "    )\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    loss = model.evaluate(X_test, y_test_reshaped, verbose=0)\n",
    "    rmse = loss**0.5\n",
    "\n",
    "    return rmse, architecture\n",
    "\n",
    "def find_best_model(\n",
    "    X,\n",
    "    y,\n",
    "    num_tries = 4, # for each number of layers in range\n",
    "    min_layers = 3,\n",
    "    max_layers = 12,\n",
    "    min_units = 8,\n",
    "    max_units = 64,\n",
    "    layer_types = ['dense'],\n",
    "    dropout_layer_types = ['dropout'],\n",
    "    dropout_chance = 0.5,\n",
    "    min_dropout_rate = 0.1,\n",
    "    max_dropout_rate = 0.5,\n",
    "    activation = 'relu',\n",
    "    epochs = 16\n",
    "):\n",
    "    if min_layers>max_layers or type(min_layers)!=int or type(max_layers)!=int:\n",
    "        print(\"ERROR: min_layers must be less than max_layers and both must be integers\")\n",
    "        return\n",
    "    \n",
    "    # Flatten the experimental space\n",
    "    tasks = [(l, t) for l in range(min_layers, max_layers + 1) for t in range(num_tries)]\n",
    "    \n",
    "    # Test all the models\n",
    "    # Runs parallel\n",
    "    # WARNING: TensorFlow and joblib (multiprocessing) often fight over the GPU.\n",
    "    # If this crashes, change n_jobs to 1.\n",
    "    results = Parallel(n_jobs=2)(\n",
    "        delayed(evaluate_candidate)(\n",
    "            X = X,\n",
    "            y = y,\n",
    "            num_layers = layers,\n",
    "            test_number = test_number,\n",
    "            layer_types = layer_types,\n",
    "            dropout_layer_types = dropout_layer_types,\n",
    "            dropout_chance = dropout_chance,\n",
    "            min_dropout_rate = min_dropout_rate,\n",
    "            max_dropout_rate = max_dropout_rate,\n",
    "            min_units = min_units,\n",
    "            max_units = max_units,\n",
    "            activation = activation,\n",
    "            epochs = epochs\n",
    "        )\n",
    "        for layers, test_number in tasks\n",
    "    )\n",
    "\n",
    "    # Find and return best model\n",
    "    best_rmse = float('inf')\n",
    "    best_architecture = None\n",
    "\n",
    "    for rmse, architecture in results:\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_architecture = architecture\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(f\"Overall Best RMSE {best_rmse}\")\n",
    "    return best_architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b98d5d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 21:25:12.446419: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-18 21:25:12.446419: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-18 21:25:12.454158: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-18 21:25:12.454185: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-18 21:25:12.824481: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-18 21:25:12.824478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-18 21:25:14.051773: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-18 21:25:14.051773: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-18 21:25:14.053567: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-18 21:25:14.053567: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 layers, attempt #1\n",
      "3 layers, attempt #2\n",
      "Cold Start: Resetting model weights to random...\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "--- Starting Epoch 1/32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-12-18 21:25:15.297674: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-12-18 21:25:15.297674: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7609912a9b20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760967731250>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c654ebdb20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c62afb9dd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 209.3341  Batch 476/499 - Loss: 462.0025\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 54/499 - Loss: 367.73831\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 160.22021\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 132/499 - Loss: 101.81241\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 362/499 - Loss: 186.5995  Batch 478/499 - Loss: 200.1263\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 110.2911\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 124.75143\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 224.4944 Batch 397/499 - Loss: 293.4150\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 139.8609  Batch 487/499 - Loss: 189.4792\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 293.5515  Batch 23/499 - Loss: 329.7499\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 99.35755  Batch 327/499 - Loss: 287.0806\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 88/499 - Loss: 112.87605  Batch 28/499 - Loss: 116.9860\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 202.42607 Batch 231/499 - Loss: 262.3344\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 199.70943\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 262.5485  Batch 263/499 - Loss: 99.19602\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 103.5702\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 122.0645  Batch 224/499 - Loss: 136.0593\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 338.9871\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 702.2092  Batch 286/499 - Loss: 216.4341\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 117.7469\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 246.2644  Batch 250/499 - Loss: 139.1184\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 77.85999\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 103.2840\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 102.3524  Batch 144/499 - Loss: 239.5119\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 307/499 - Loss: 104.97762 Batch 69/499 - Loss: 108.2794\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 57.47813  Batch 160/499 - Loss: 132.7400\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 285.6016  Batch 357/499 - Loss: 160.5204\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 100.3742  Batch 277/499 - Loss: 207.5661\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 87.4820  Batch 204/499 - Loss: 136.5486\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 166.7573  Batch 471/499 - Loss: 133.6645\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 167.6442 Batch 438/499 - Loss: 171.0613\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 389/499 - Loss: 212.3161  Batch 479/499 - Loss: 330.3441\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 110.9274\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 132.4421  Batch 465/499 - Loss: 103.7622\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 263.6134\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 491/499 - Loss: 131.4598  Batch 354/499 - Loss: 168.9658\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 174.2340\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 27.44797  Batch 286/499 - Loss: 99.08511\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 45.84113\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 102.8163  Batch 484/499 - Loss: 216.9416\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 183.1958\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 153.3506\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 53.14441\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 479/499 - Loss: 168.3149 Batch 437/499 - Loss: 219.67355\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 150.1675\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 478/499 - Loss: 207.9886  Batch 441/499 - Loss: 490.6046\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 234.5146\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 215.6814  Batch 451/499 - Loss: 139.0316\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 34/499 - Loss: 38.18850  Batch 498/499 - Loss: 136.4539\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 104.7867  Batch 439/499 - Loss: 112.8293\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 137.3434\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 68.31125  Batch 176/499 - Loss: 142.3671\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 402.9331\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 81.387792 Batch 297/499 - Loss: 108.6982\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 63.55119  Batch 17/499 - Loss: 74.7051\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 487/499 - Loss: 62.80807  Batch 160/499 - Loss: 133.8810\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 106.6402\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 160.2050  Batch 338/499 - Loss: 131.5569\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 11.55309  Batch 8/499 - Loss: 331.6575\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 202.0130  Batch 421/499 - Loss: 181.8348\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 135.3943\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 287/499 - Loss: 136.4861  Batch 300/499 - Loss: 282.1440\n",
      "Training Complete.\n",
      "\n",
      "best loss 19.384294509887695\n",
      "  Batch 499/499 - Loss: 69.31478\n",
      "--- Starting Epoch 32/32 ---\n",
      "3 layers, attempt #3ss: 112.2585\n",
      "  Batch 499/499 - Loss: 490.3215\n",
      "Training Complete.\n",
      "\n",
      "best loss 11.553006172180176\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c62af27b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c62af324d0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 layers, attempt #4ss: 213.3960\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.8581\n",
      "  Batch 499/499 - Loss: 171.5265\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 99/499 - Loss: 503.10807\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76096732bb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7609673dd110>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 320.44977 Batch 215/499 - Loss: 397.7900\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 62/499 - Loss: 211.1172\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 446/499 - Loss: 244.2232 Batch 493/499 - Loss: 221.34392\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 834.5176\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 342.6143  Batch 271/499 - Loss: 386.4404\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 396.5156\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 214.8039  Batch 425/499 - Loss: 122.9057\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 59.90226\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 115.8188  Batch 484/499 - Loss: 360.8426\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 88.55227\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 308.7598  Batch 339/499 - Loss: 196.0241\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 377.15705\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 199.4431  Batch 195/499 - Loss: 304.9174\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 208/499 - Loss: 105.8100  Batch 194/499 - Loss: 180.1334\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 279/499 - Loss: 171.9445  Batch 230/499 - Loss: 513.7454\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 651.5971  Batch 12/499 - Loss: 261.3339\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 177.4068  Batch 220/499 - Loss: 195.4614\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 240.7704  Batch 77/499 - Loss: 330.6125\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 155.9741  Batch 457/499 - Loss: 300.4923\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 161.4850\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 139.0459  Batch 254/499 - Loss: 191.7320\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 225.4853  Batch 23/499 - Loss: 313.4393\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 281.1536  Batch 376/499 - Loss: 82.2243\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 217.13493Batch 296/499 - Loss: 192.1478\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 212/499 - Loss: 257.4623 Batch 374/499 - Loss: 378.8304\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 42.43547  Batch 424/499 - Loss: 265.0534\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 124.0691  Batch 423/499 - Loss: 274.7819\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 123.5983  Batch 239/499 - Loss: 149.1262\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 237.8414Batch 347/499 - Loss: 212.0199\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 62.97044  Batch 210/499 - Loss: 270.1091\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 168.2317  Batch 136/499 - Loss: 295.3215\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 428.5340  Batch 347/499 - Loss: 224.7275\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 154.6633\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 282.2612  Batch 402/499 - Loss: 439.5287\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 281.5199  Batch 49/499 - Loss: 247.3396\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 106.8686  Batch 393/499 - Loss: 157.8409\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 230.9642\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 40.93949 Batch 318/499 - Loss: 126.17913\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 158.8391\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 449.1301  Batch 232/499 - Loss: 164.9809\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 82.82438\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 98.02014  Batch 410/499 - Loss: 122.4917\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 24/499 - Loss: 147.39374\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 367/499 - Loss: 173.3299  Batch 337/499 - Loss: 627.9723\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 79.29317  Batch 102/499 - Loss: 176.5948\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 154.3617 Batch 183/499 - Loss: 144.0444\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 27.25764  Batch 34/499 - Loss: 533.5869\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 351/499 - Loss: 193.7502  Batch 284/499 - Loss: 224.4283  Batch 179/499 - Loss: 243.8698\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 801.5143  Batch 94/499 - Loss: 92.4181\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 150.1469  Batch 217/499 - Loss: 251.8827\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 161/499 - Loss: 249.1711\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 267.2997  Batch 327/499 - Loss: 184.1521\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 86.09349  Batch 28/499 - Loss: 248.6461\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 130.3011  Batch 105/499 - Loss: 208.6106\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 56.36631\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 69.16151  Batch 491/499 - Loss: 404.4133\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 347.4962 Batch 493/499 - Loss: 166.5203\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 377/499 - Loss: 160.92443\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 104.2550\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 305.8058  Batch 271/499 - Loss: 88.23597\n",
      "Training Complete.\n",
      "\n",
      "best loss 32.85728073120117\n",
      "  Batch 499/499 - Loss: 233.3268\n",
      "--- Starting Epoch 31/32 ---\n",
      "3 layers, attempt #5ss: 156.3933\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.7105\n",
      "  Batch 258/499 - Loss: 181.0467\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c62af27ce0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c628148910>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 399.3385 Batch 201/499 - Loss: 451.48603\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 324/499 - Loss: 115.1233  Batch 490/499 - Loss: 323.8341\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 127.5896  Batch 119/499 - Loss: 464.1445\n",
      "Training Complete.\n",
      "\n",
      "best loss 14.798270225524902\n",
      "3 layers, attempt #6ss: 245.48263\n",
      "  Batch 499/499 - Loss: 406.3718\n",
      "--- Starting Epoch 3/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---0189\n",
      "  Batch 202/499 - Loss: 318.2921\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76096732bec0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76095c5c1450>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 230.5683  Batch 450/499 - Loss: 328.8481\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 91.71364\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 346.4162  Batch 456/499 - Loss: 122.3930\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 317.17108 Batch 250/499 - Loss: 233.0444\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 91.08833  Batch 61/499 - Loss: 294.2663\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 199.0954  Batch 328/499 - Loss: 197.7964\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 89.66029\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 82.405595\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 298.17096Batch 370/499 - Loss: 212.6993\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 317/499 - Loss: 250.8081  Batch 171/499 - Loss: 315.3016\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 390.8468  Batch 105/499 - Loss: 157.4302\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 437.7373  Batch 292/499 - Loss: 72.3456\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 178.1919  Batch 492/499 - Loss: 345.3523\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 261.7912  Batch 451/499 - Loss: 325.7614\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 125/499 - Loss: 100.4411  Batch 83/499 - Loss: 145.9437\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 391.42127 Batch 50/499 - Loss: 267.4286\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 145.8138  Batch 492/499 - Loss: 102.8101\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 289.4166  Batch 497/499 - Loss: 246.2868\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 695.7842  Batch 63/499 - Loss: 165.1059\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 234.5171  Batch 219/499 - Loss: 197.7591\n",
      "  Batch 388/499 - Loss: 215.9012--- Starting Epoch 11/32 ---\n",
      "  Batch 109/499 - Loss: 207.7290\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 400/499 - Loss: 475.1956  Batch 274/499 - Loss: 249.1272\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 412.0645  Batch 31/499 - Loss: 377.7415\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 378/499 - Loss: 327.7615 Batch 245/499 - Loss: 305.22508\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 189.5179Batch 388/499 - Loss: 270.6377\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 139.0401\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 237.4976\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 366/499 - Loss: 192.8395  Batch 374/499 - Loss: 128.8273\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 113/499 - Loss: 107.3848  Batch 84/499 - Loss: 224.6992\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 420/499 - Loss: 196.6870  Batch 325/499 - Loss: 308.6656\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 117.5290  Batch 13/499 - Loss: 167.7334\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 295.2832  Batch 418/499 - Loss: 88.93046\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 319.4582  Batch 14/499 - Loss: 148.5112\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 62.286325 Batch 410/499 - Loss: 128.5434\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 193.6573 Batch 426/499 - Loss: 237.3781\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 225.3101  Batch 447/499 - Loss: 97.49325\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 129.2067 Batch 108/499 - Loss: 161.6700\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 345/499 - Loss: 259.1224  Batch 165/499 - Loss: 155.2001\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 123.6620\n",
      "  Batch 149/499 - Loss: 264.5124--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 50.44539  Batch 493/499 - Loss: 117.9482\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 227.7404  Batch 121/499 - Loss: 90.5296\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 260.4592  Batch 439/499 - Loss: 164.8197\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 201/499 - Loss: 113.30533 Batch 39/499 - Loss: 173.6274\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 125.0543  Batch 421/499 - Loss: 100.3306\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 1308.3735Batch 434/499 - Loss: 171.1945\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 104.7859  Batch 471/499 - Loss: 80.13753\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 257.4376  Batch 94/499 - Loss: 211.6420\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 244.9826  Batch 281/499 - Loss: 125.3223\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 145/499 - Loss: 53.62467  Batch 143/499 - Loss: 126.7955\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 230.0116  Batch 195/499 - Loss: 198.9093\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 107.0906  Batch 72/499 - Loss: 146.7997\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 370/499 - Loss: 319.9979 Batch 201/499 - Loss: 105.0786\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 281.2627Batch 475/499 - Loss: 314.81244  Batch 98/499 - Loss: 95.6217\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 232.0250  Batch 254/499 - Loss: 336.4202\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 73.13394\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 380.3955 Batch 327/499 - Loss: 135.5169\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 185.9376 Batch 488/499 - Loss: 200.5384\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 130.4163\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 65.33037\n",
      "Training Complete.\n",
      "\n",
      "best loss 43.08592224121094\n",
      "4 layers, attempt #1ss: 89.05246\n",
      "  Batch 499/499 - Loss: 79.09403\n",
      "--- Starting Epoch 31/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---2288\n",
      "  Batch 102/499 - Loss: 187.5954\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c62af8ec00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c620115d10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 222/499 - Loss: 813.73081\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 369/499 - Loss: 156.94026\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 149.6562\n",
      "Training Complete.\n",
      "\n",
      "best loss 24.115585327148438\n",
      "4 layers, attempt #2ss: 226.73932\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.96015\n",
      "  Batch 499/499 - Loss: 358.1481\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 27/499 - Loss: 70.20820\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7609646054e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76095c079750>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 316.91723 Batch 394/499 - Loss: 525.77973\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 251.7689 Batch 443/499 - Loss: 86.78862\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 182.5388  Batch 495/499 - Loss: 221.7167\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 116.7931  Batch 436/499 - Loss: 138.1900\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 121.07733\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 202.80869 Batch 233/499 - Loss: 320.8488\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 69.09192  Batch 220/499 - Loss: 346.1210\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 264.3252  Batch 139/499 - Loss: 122.9255\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 114.93248Batch 247/499 - Loss: 117.5979\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 673.3338  Batch 138/499 - Loss: 250.1003\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 435.1578 Batch 311/499 - Loss: 362.8258\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 53.34878  Batch 194/499 - Loss: 172.3232\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 54.97268 Batch 179/499 - Loss: 223.8202\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 255.3022 Batch 454/499 - Loss: 264.4215\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 184.6286  Batch 235/499 - Loss: 133.9881\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 134.1008  Batch 441/499 - Loss: 290.7937\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 446.1667  Batch 233/499 - Loss: 148.5053\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 136.3876  Batch 485/499 - Loss: 577.0528\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 99.58167\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 184.4709  Batch 49/499 - Loss: 230.5849\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 321/499 - Loss: 199.48920Batch 487/499 - Loss: 980.6348\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 218.8808\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 330/499 - Loss: 175.1214  Batch 305/499 - Loss: 79.5268\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 294.8501 Batch 70/499 - Loss: 182.62168\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 319/499 - Loss: 245.2612  Batch 183/499 - Loss: 209.7589\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 69.36492  Batch 74/499 - Loss: 229.1355\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 231.0408  Batch 357/499 - Loss: 333.2466\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 124/499 - Loss: 175.9885\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 279.5425  Batch 408/499 - Loss: 142.2457\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 63.52940  Batch 488/499 - Loss: 180.6109\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 86.00784  Batch 274/499 - Loss: 70.6585\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 162.6686  Batch 150/499 - Loss: 268.1630\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 207.9656  Batch 480/499 - Loss: 171.0584\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 120.5059 Batch 343/499 - Loss: 100.8862\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 215.6739 Batch 468/499 - Loss: 291.1167\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 186.8542\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 116.5496  Batch 275/499 - Loss: 250.4731\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 134.2961\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 300/499 - Loss: 253.7073\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 56.51438\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 315/499 - Loss: 87.76478\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 95.43067 Batch 366/499 - Loss: 191.6238\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 261.0996  Batch 481/499 - Loss: 179.9321\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 33.38120  Batch 443/499 - Loss: 93.5649\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 336/499 - Loss: 118.5730  Batch 381/499 - Loss: 72.36722\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 83.51873 Batch 120/499 - Loss: 216.46609\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 189.8619 Batch 319/499 - Loss: 86.815174\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 195.8550\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 231.7505  Batch 345/499 - Loss: 155.5849\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 133.4814 Batch 452/499 - Loss: 75.05670\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 117.7362  Batch 403/499 - Loss: 132.0536\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 156.8127 Batch 438/499 - Loss: 123.1968\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 62.47316  Batch 239/499 - Loss: 118.3955\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 323.8508\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 177.9367  Batch 320/499 - Loss: 98.5292\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 163.0271  Batch 45/499 - Loss: 121.2270\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 396/499 - Loss: 308.1810\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 126.6326\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 334.4948 Batch 436/499 - Loss: 152.50905\n",
      "Training Complete.\n",
      "\n",
      "best loss 20.535627365112305\n",
      "  Batch 499/499 - Loss: 24.84391\n",
      "--- Starting Epoch 31/32 ---\n",
      "4 layers, attempt #3ss: 136.6227\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.0755\n",
      "  Batch 421/499 - Loss: 167.9262\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c62af8f6a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c6044b2d90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 149.4270\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 290.88823 Batch 192/499 - Loss: 94.8777\n",
      "Training Complete.\n",
      "\n",
      "best loss 19.033079147338867\n",
      "  Batch 499/499 - Loss: 205.7173\n",
      "--- Starting Epoch 2/32 ---\n",
      "4 layers, attempt #4ss: 144.50834\n",
      "  Batch 211/499 - Loss: 348.0355\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x760967390cc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76095c5d1d10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.2253\n",
      "  Batch 499/499 - Loss: 139.7613  Batch 38/499 - Loss: 1622.5215\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 91.036374 Batch 324/499 - Loss: 1153.4556\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 218.6499\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 140.61033 Batch 445/499 - Loss: 422.5224\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 1750.4598\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 495/499 - Loss: 249.7168  Batch 499/499 - Loss: 34.49267\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 612.4539\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 111.5759  Batch 251/499 - Loss: 381.3950\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 128.6435\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 582.98599 Batch 287/499 - Loss: 165.2657\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 249.6440\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 214.8802  Batch 307/499 - Loss: 288.9707\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 55/499 - Loss: 186.67083\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 226.2924  Batch 395/499 - Loss: 264.2080\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 38.94197\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 371/499 - Loss: 116.0571 Batch 117/499 - Loss: 313.3326\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 258.3734\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 79.79714\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 171/499 - Loss: 614.4473  Batch 170/499 - Loss: 337.3793\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 101.0960\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 203/499 - Loss: 317.4328  Batch 499/499 - Loss: 191.9645\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 284/499 - Loss: 189.5300 Batch 235/499 - Loss: 374.4789\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 199/499 - Loss: 194.2663  Batch 15/499 - Loss: 228.8322\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 304/499 - Loss: 272.7220 Batch 294/499 - Loss: 438.4174\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 185/499 - Loss: 281.4926\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 277.0134\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 56.39558  Batch 228/499 - Loss: 184.7594\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 113.1953  Batch 440/499 - Loss: 323.5464\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 210.5362  Batch 454/499 - Loss: 106.5246\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 164/499 - Loss: 136.4096  Batch 82/499 - Loss: 184.1585\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 351/499 - Loss: 225.0720  Batch 232/499 - Loss: 132.3939\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 164/499 - Loss: 119.8418\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 335.1405  Batch 367/499 - Loss: 155.5170\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 78/499 - Loss: 166.78488 Batch 477/499 - Loss: 253.9968\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 345.6982  Batch 434/499 - Loss: 213.3427\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 249.3801\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 469/499 - Loss: 280.9414 Batch 428/499 - Loss: 120.75140\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 482.6066 Batch 489/499 - Loss: 314.0144\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 68.85223  Batch 341/499 - Loss: 193.2152\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 214.7113\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 141.3959  Batch 357/499 - Loss: 241.3151\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 44/499 - Loss: 97.536159 Batch 26/499 - Loss: 130.6491\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 494/499 - Loss: 454.3896  Batch 379/499 - Loss: 97.78400\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 291.6373\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 144.1339  Batch 360/499 - Loss: 198.2586\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 62/499 - Loss: 121.77135\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 163.6473  Batch 428/499 - Loss: 101.4441\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 42/499 - Loss: 109.74175 Batch 455/499 - Loss: 182.3586\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 104.1697  Batch 298/499 - Loss: 340.1122\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 44.46123\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 261.5629  Batch 481/499 - Loss: 255.9351\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 256.7903\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 217.8217  Batch 451/499 - Loss: 99.2035\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 99.85644\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 487/499 - Loss: 240.1809  Batch 486/499 - Loss: 92.3586\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 248.6062\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 51.11303  Batch 426/499 - Loss: 121.8065\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 209.3653\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 173.0383 Batch 489/499 - Loss: 143.48949\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 77/499 - Loss: 197.02262\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 141.9856  Batch 378/499 - Loss: 284.8534\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 126.6647\n",
      "Training Complete.\n",
      "\n",
      "best loss 15.96109676361084\n",
      "  Batch 499/499 - Loss: 395.8221\n",
      "Training Complete.\n",
      "\n",
      "best loss 17.85035514831543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c604450180> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5f4619250>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layers, attempt #5\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "4 layers, attempt #6s: 1891.9224\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.65840\n",
      "  Batch 319/499 - Loss: 557.04804\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x760967393b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76095c0ed510>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 236/499 - Loss: 417.81517\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 183.54935\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 118/499 - Loss: 267.5886  Batch 497/499 - Loss: 329.4193\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 372/499 - Loss: 783.6569  Batch 438/499 - Loss: 332.2370\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 188.56508Batch 423/499 - Loss: 171.5246\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 57.318060 Batch 355/499 - Loss: 125.2843\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 128.70241\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 535.95142 Batch 447/499 - Loss: 469.7013\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 259.7781\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 215.02676 Batch 280/499 - Loss: 318.3812\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 306.52823atch 342/499 - Loss: 220.6974\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 200.69882 Batch 213/499 - Loss: 247.2349\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 575.20550 Batch 78/499 - Loss: 345.1805\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 93.29463  Batch 333/499 - Loss: 449.1660\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 171.4827  Batch 142/499 - Loss: 172.4395\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 370.6433  Batch 286/499 - Loss: 201.1527\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 223.25273\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 96.692928 Batch 178/499 - Loss: 408.0383\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 147.5633\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 369.8555  Batch 283/499 - Loss: 169.0036\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 270.8022 Batch 463/499 - Loss: 178.0341\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 311.09326Batch 267/499 - Loss: 173.8261\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 275/499 - Loss: 192.46452 Batch 127/499 - Loss: 285.8113\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 631.0037 Batch 377/499 - Loss: 257.5316\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 339/499 - Loss: 233.7280 Batch 169/499 - Loss: 161.98078\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 160.9263\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 145.4500  Batch 455/499 - Loss: 212.8084\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 36.28800\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 278.5266  Batch 344/499 - Loss: 529.0188\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 438.1178\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 390/499 - Loss: 195.5725  Batch 428/499 - Loss: 289.5955\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 361.54878 Batch 18/499 - Loss: 297.4870\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 200.0713  Batch 290/499 - Loss: 253.3550\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 129.2402  Batch 78/499 - Loss: 187.8090\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 554.65025\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 272.9601  Batch 165/499 - Loss: 188.9688\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 111.7914  Batch 217/499 - Loss: 351.9731\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 47.654981Batch 420/499 - Loss: 91.58610\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 180.2090  Batch 163/499 - Loss: 147.0977\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 33.12974 Batch 320/499 - Loss: 202.5626\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 269/499 - Loss: 205.11339 Batch 164/499 - Loss: 148.7187\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 112.5083\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 37.317706Batch 121/499 - Loss: 177.6018\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 140.8256  Batch 476/499 - Loss: 215.8990\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 129.7193  Batch 126/499 - Loss: 253.9379\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 71.84424  Batch 134/499 - Loss: 181.2325\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 386.53522\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 312/499 - Loss: 168.0999  Batch 457/499 - Loss: 192.6284\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 56.11310Batch 333/499 - Loss: 198.1224\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 158.6105  Batch 264/499 - Loss: 275.6659\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 72.59311 Batch 378/499 - Loss: 248.2541  Batch 379/499 - Loss: 313.6589\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 490/499 - Loss: 152.5825  Batch 442/499 - Loss: 542.8534\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 72.87315\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 108.4500  Batch 290/499 - Loss: 163.6199\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 72.21711\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 353/499 - Loss: 150.2504\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 179.2581\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 348/499 - Loss: 251.4112  Batch 325/499 - Loss: 256.8655\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 158.75455\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 326/499 - Loss: 110.8826  Batch 499/499 - Loss: 104.8907\n",
      "Training Complete.\n",
      "\n",
      "best loss 26.163110733032227\n",
      "  Batch 499/499 - Loss: 178.9904\n",
      "--- Starting Epoch 30/32 ---\n",
      "5 layers, attempt #1ss: 133.9522\n",
      "  Batch 418/499 - Loss: 205.9639\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c6282076a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5f45a9450>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.9516\n",
      "  Batch 499/499 - Loss: 119.7118\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 26.27565  Batch 498/499 - Loss: 356.72181\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 433.15120\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 175.67810 Batch 122/499 - Loss: 304.0582\n",
      "Training Complete.\n",
      "\n",
      "best loss 24.515485763549805\n",
      "  Batch 499/499 - Loss: 786.6611\n",
      "--- Starting Epoch 3/32 ---\n",
      "5 layers, attempt #2s: 154.4815\n",
      "  Batch 110/499 - Loss: 739.5074\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76095c5c5580> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7609673dfd50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.7097\n",
      "  Batch 499/499 - Loss: 77.851223\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 475.5454  Batch 464/499 - Loss: 376.4680\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 801.5778\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 288.99484 Batch 240/499 - Loss: 283.5146\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 468.1802  Batch 143/499 - Loss: 288.0014\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 435/499 - Loss: 387.41027Batch 210/499 - Loss: 192.6179\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 196.1190\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 612.49210 Batch 414/499 - Loss: 281.5394\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 138.6239\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 441.05668\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 232.3452\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 346.18128 Batch 301/499 - Loss: 385.3219\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 178.5838  Batch 115/499 - Loss: 458.6734\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 403.0846  Batch 343/499 - Loss: 478.1840\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 236/499 - Loss: 348.60578\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 217/499 - Loss: 787.0765  Batch 111/499 - Loss: 351.4490\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 364/499 - Loss: 362.8808  Batch 226/499 - Loss: 322.1042\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 200.9082\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 857.52949 Batch 200/499 - Loss: 183.9815\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 416.53788Batch 369/499 - Loss: 171.9096\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 345.1713  Batch 132/499 - Loss: 170.9545\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 231.94601 Batch 163/499 - Loss: 159.8606\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 542.0357\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 95.740015\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 164.5971  Batch 197/499 - Loss: 188.2845\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 410.9273  Batch 239/499 - Loss: 144.6570\n",
      "  Batch 248/499 - Loss: 233.0475--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 364.70176\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 95.72348  Batch 452/499 - Loss: 154.5194\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 400.0242  Batch 281/499 - Loss: 468.4205\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 144/499 - Loss: 160.5805  Batch 63/499 - Loss: 271.8653\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 385.0167  Batch 274/499 - Loss: 312.0429\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 168/499 - Loss: 345.2226  Batch 454/499 - Loss: 302.6492\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 76.47880  Batch 487/499 - Loss: 97.19016\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 84.04288\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 59.90906  Batch 107/499 - Loss: 151.8751\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 473.1587  Batch 350/499 - Loss: 265.7587\n",
      "--- Starting Epoch 22/32 ---3885\n",
      "  Batch 48/499 - Loss: 159.73949\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 419/499 - Loss: 133.3207  Batch 418/499 - Loss: 305.0365\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 151.8161Batch 431/499 - Loss: 292.1883\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 362/499 - Loss: 172.8917  Batch 168/499 - Loss: 433.6134\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 165.9551  Batch 71/499 - Loss: 277.4431\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 159.0401  Batch 164/499 - Loss: 237.3480\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 202.8937  Batch 95/499 - Loss: 217.0738\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 365.37324 Batch 195/499 - Loss: 166.1200\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 267.8307\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 428/499 - Loss: 398.8517  Batch 358/499 - Loss: 111.1001\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 164.4852  Batch 33/499 - Loss: 414.9526\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 423.6871  Batch 478/499 - Loss: 166.5744\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 274.6389\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 96.50662  Batch 496/499 - Loss: 359.2701\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 254.8680\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 554.1944  Batch 374/499 - Loss: 142.8025\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 513.5980\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 202.4796  Batch 383/499 - Loss: 152.1963\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 121.8910  Batch 228/499 - Loss: 162.0835\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 102.8239  Batch 459/499 - Loss: 475.0129\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 391.9306  Batch 356/499 - Loss: 98.52233\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 527.7320  Batch 22/499 - Loss: 268.6980\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 440/499 - Loss: 172.3295  Batch 469/499 - Loss: 190.8392\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 281.9027 Batch 470/499 - Loss: 243.6957\n",
      "Training Complete.\n",
      "\n",
      "best loss 35.751861572265625\n",
      "  Batch 499/499 - Loss: 58.093259\n",
      "--- Starting Epoch 32/32 ---\n",
      "5 layers, attempt #3s: 112.3177\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.3791\n",
      "  Batch 187/499 - Loss: 121.0195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5f46071a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c62afdbbd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 378.7421  Batch 40/499 - Loss: 981.1482\n",
      "Training Complete.\n",
      "\n",
      "best loss 41.71079635620117\n",
      "5 layers, attempt #4ss: 818.24213\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.6353\n",
      "  Batch 457/499 - Loss: 453.34530\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76095c5c59e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76091406d050>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 372.89554\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 422.07951 Batch 367/499 - Loss: 414.8865\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 246/499 - Loss: 566.86305 Batch 87/499 - Loss: 258.4730\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 145.6099  Batch 463/499 - Loss: 365.3837\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 1166.3867\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 438.84491Batch 431/499 - Loss: 311.2977\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 337.8789  Batch 470/499 - Loss: 237.1755\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 197.6707\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 311.6854  Batch 458/499 - Loss: 289.5242\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 181.54618\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 472/499 - Loss: 291.43140 Batch 17/499 - Loss: 193.6697\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 206.2412\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 534.7465  Batch 460/499 - Loss: 313.0151\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 296.0550\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 269.2731  Batch 419/499 - Loss: 345.2230\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 456.84925\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 100.4186  Batch 472/499 - Loss: 203.4859  Batch 458/499 - Loss: 490.8583\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 222.9346\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 429/499 - Loss: 311.9518  Batch 398/499 - Loss: 396.7454\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 128/499 - Loss: 146.1429  Batch 72/499 - Loss: 269.0489\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 284.53456Batch 248/499 - Loss: 349.9752\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 117.1037 Batch 366/499 - Loss: 370.8801\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 178.9165  Batch 466/499 - Loss: 118.8835\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 248.0606  Batch 484/499 - Loss: 340.0877\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 263.41239\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 58/499 - Loss: 188.45169\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 237.76651 Batch 246/499 - Loss: 189.6996\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 50.20613\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 327.1849  Batch 379/499 - Loss: 574.9020\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 98.16094\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 184.7540  Batch 169/499 - Loss: 240.9846\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 623.9232\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 162.2495  Batch 178/499 - Loss: 415.4557\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 115/499 - Loss: 73.16949 Batch 477/499 - Loss: 385.0840\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 174.56421 Batch 351/499 - Loss: 359.0175\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 240.4313\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 177.7287  Batch 389/499 - Loss: 312.8454\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 131/499 - Loss: 251.3453  Batch 88/499 - Loss: 203.2505\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 366.9379  Batch 294/499 - Loss: 332.5627\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 223.0204  Batch 492/499 - Loss: 248.3530\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 234.2409  Batch 71/499 - Loss: 201.8549\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 197.09477 Batch 357/499 - Loss: 331.4682\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 244.0202  Batch 207/499 - Loss: 389.3486\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 721.0764  Batch 193/499 - Loss: 107.8157\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 310.98848 Batch 2/499 - Loss: 154.4608\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 346/499 - Loss: 225.18507\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 143.50151\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 361.7539  Batch 147/499 - Loss: 171.6560\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 500.84124 Batch 374/499 - Loss: 260.2614\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 718.5639  Batch 24/499 - Loss: 135.2223\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 313/499 - Loss: 365.33047\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 224.8225 Batch 349/499 - Loss: 255.1140\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 294.7561\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 447.3631  Batch 175/499 - Loss: 197.8639\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 136/499 - Loss: 449.45051\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 528.3257  Batch 378/499 - Loss: 129.5165\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 102.2874\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 163.28947Batch 265/499 - Loss: 136.91502\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 376.8927 Batch 485/499 - Loss: 111.5275\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 421/499 - Loss: 240.2721  Batch 400/499 - Loss: 247.7808\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 132/499 - Loss: 214.0162\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 574.0703  Batch 327/499 - Loss: 231.6972\n",
      "Training Complete.\n",
      "\n",
      "best loss 43.82110595703125\n",
      "  Batch 499/499 - Loss: 247.5775\n",
      "--- Starting Epoch 32/32 ---\n",
      "5 layers, attempt #5ss: 113.1170\n",
      "  Batch 499/499 - Loss: 133.4118\n",
      "Training Complete.\n",
      "\n",
      "best loss 32.01884460449219\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c604453c40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5cc0ea790>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 layers, attempt #6s: 2145.1814\n",
      "  Batch 146/499 - Loss: 1756.4445\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76095c082980> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760914103010>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.9620\n",
      "  Batch 499/499 - Loss: 344.20467\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 59.485675\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 871.7933\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 413.29962 Batch 379/499 - Loss: 321.95348\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 215.5206\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 31.962016 Batch 381/499 - Loss: 502.37521\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 485.44702\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 199.51806 Batch 203/499 - Loss: 134.5682\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 130/499 - Loss: 105.37814\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 391.88676 Batch 222/499 - Loss: 407.9198\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 208/499 - Loss: 328.46613 Batch 420/499 - Loss: 655.0186\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 258/499 - Loss: 363.70893 Batch 81/499 - Loss: 471.2815\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 291/499 - Loss: 196.20328 Batch 227/499 - Loss: 159.9782\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 81.67510  Batch 114/499 - Loss: 591.9037\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 555.32889 Batch 362/499 - Loss: 171.7563\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 110.34392\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 590.99189  Batch 316/499 - Loss: 130.4338\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 146/499 - Loss: 1275.2330Batch 442/499 - Loss: 570.0581\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 339.28123 Batch 162/499 - Loss: 214.5947\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 124.41079 Batch 442/499 - Loss: 322.9897\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 673.54222 Batch 203/499 - Loss: 275.3618\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 368.13757 Batch 133/499 - Loss: 692.2034\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 666.29185 Batch 304/499 - Loss: 146.3053\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 185.4979  Batch 72/499 - Loss: 440.4736\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 456/499 - Loss: 128.86232 Batch 52/499 - Loss: 317.6155\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 69/499 - Loss: 487.66441\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 387.71392 Batch 388/499 - Loss: 353.1835\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 38.83726\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 398.62182 Batch 235/499 - Loss: 772.9194\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 87/499 - Loss: 421.49855\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 903.91745\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 785.3449  Batch 56/499 - Loss: 577.3896\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 471/499 - Loss: 157.8794  Batch 467/499 - Loss: 138.8561\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 61.31461\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 177.81522Batch 82/499 - Loss: 745.6600\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 223.00795\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 32.898271 Batch 405/499 - Loss: 225.5393\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 330.14024\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 149.64809 Batch 362/499 - Loss: 951.8836\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 48.75347\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 214.66563 Batch 431/499 - Loss: 207.9263\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 1027.6102\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 12.056487\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 1236.5204 Batch 483/499 - Loss: 1213.5387\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 53.190196 Batch 81/499 - Loss: 484.8224\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 695.20120Batch 430/499 - Loss: 1045.2192\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 507.09096 Batch 197/499 - Loss: 215.3096\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 353.15599\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 120.99805 Batch 64/499 - Loss: 690.4841\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 231/499 - Loss: 96.298713 Batch 420/499 - Loss: 457.6452\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 186.8909  Batch 493/499 - Loss: 114.0176\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 453.21809\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 792.05806 Batch 352/499 - Loss: 406.4073\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 180.8596\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 455/499 - Loss: 289.79709 Batch 471/499 - Loss: 544.1635\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 207.6070  Batch 24/499 - Loss: 633.7846\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 414/499 - Loss: 923.36658Batch 151/499 - Loss: 835.8511\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 82/499 - Loss: 225.383480\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 350.43300Batch 437/499 - Loss: 630.64953\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1268.3217\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 494/499 - Loss: 281.76760 Batch 469/499 - Loss: 764.3986\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 203.2816\n",
      "Training Complete.\n",
      "\n",
      "best loss 12.056429862976074\n",
      "6 layers, attempt #1ss: 408.97059\n",
      "  Batch 410/499 - Loss: 1332.6313\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c62af8cb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c628161450>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "  Batch 499/499 - Loss: 671.90709\n",
      "--- Starting Epoch 32/32 ---\n",
      "--- Starting Epoch 1/32 ---554\n",
      "  Batch 499/499 - Loss: 462.33095 Batch 164/499 - Loss: 330.4481\n",
      "Training Complete.\n",
      "\n",
      "best loss 119.49762725830078\n",
      "6 layers, attempt #2ss: 328.06319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "  Batch 499/499 - Loss: 668.3799\n",
      "--- Starting Epoch 2/32 ---\n",
      "--- Starting Epoch 1/32 ---7773\n",
      "  Batch 145/499 - Loss: 286.70240\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76096732ba60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7609645c3410>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 528.01646 Batch 384/499 - Loss: 268.9053\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 639.64705 Batch 152/499 - Loss: 210.2111\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 17/499 - Loss: 234.86860\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 397.39009 Batch 312/499 - Loss: 392.5513\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 18/499 - Loss: 554.81529\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 402/499 - Loss: 521.56712 Batch 448/499 - Loss: 286.8496\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 369.3723  Batch 19/499 - Loss: 1359.1000\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 179.09772 Batch 264/499 - Loss: 441.1205\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 249.1697\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 272.9580  Batch 181/499 - Loss: 436.8811\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 471.5799\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 155.77537\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 146/499 - Loss: 227.5434\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 621.1695  Batch 212/499 - Loss: 339.6064\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 230.8006Batch 345/499 - Loss: 183.7973\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 139.64875\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 380/499 - Loss: 278.46362Batch 231/499 - Loss: 287.8584\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 338.21809\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 438/499 - Loss: 219.28644 Batch 373/499 - Loss: 381.3461\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 117/499 - Loss: 363.0427\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 249.0098  Batch 296/499 - Loss: 362.8761\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 201.7385  Batch 116/499 - Loss: 463.4177\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 133.87758 Batch 359/499 - Loss: 171.5194\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 123.32466\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 276.7402  Batch 489/499 - Loss: 286.0233\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 102.9639  Batch 476/499 - Loss: 456.1926\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 303.6532  Batch 320/499 - Loss: 461.3174\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 171.5463  Batch 474/499 - Loss: 229.6833\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 541.60644 Batch 398/499 - Loss: 481.1532\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 111.3653\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 103.8139  Batch 179/499 - Loss: 237.6882\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 254.6236  Batch 196/499 - Loss: 87.78075\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 216.64794 Batch 52/499 - Loss: 346.7124\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 116.31145\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 51.17449  Batch 234/499 - Loss: 210.9642\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan.3930382217510912.0000\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 367.20294h 277/499 - Loss: 533.0941\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan3117\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 102.0160ch 481/499 - Loss: 579.3462\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan.0341\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 250.3186  Batch 293/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan7252\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 174.01183h 288/499 - Loss: 444.5892\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan.9885\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 272.7385  Batch 52/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan3433tch 4/499 - Loss: 290.1522\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan.59291h 436/499 - Loss: 228.5787\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 260.0881\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 444/499 - Loss: 236.4132\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 190.52262\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan.8448  Batch 490/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 162.7865\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan.1903  Batch 493/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 72.78675ch 474/499 - Loss: 229.6259\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan.4992ch 270/499 - Loss: 150.4482\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 257.56046 427/499 - Loss: 179.5657\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan.6149  Batch 497/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 574.9207  Batch 63/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 420/499 - Loss: 621.5288ch 122/499 - Loss: 168.6990\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 78.570802\n",
      "Training Complete.\n",
      "\n",
      "best loss 44.60613250732422\n",
      "6 layers, attempt #3ss: nan\n",
      "  Batch 351/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c62af8eb60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5cc137cd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan.55712h 54/499 - Loss: 2868.5625\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 600.2696  Batch 251/499 - Loss: nan\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan56167\n",
      "Training Complete.\n",
      "  Batch 98/499 - Loss: 473.3525\n",
      "best loss 44.330902099609375\n",
      "6 layers, attempt #4ss: 362.83084\n",
      "  Batch 491/499 - Loss: 603.3395\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 1068.6099\n",
      "--- Starting Epoch 3/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---3036\n",
      "  Batch 120/499 - Loss: 541.9406\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76096732bf60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608f4515510>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 293.91223 Batch 401/499 - Loss: 377.2530\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 508.26932 Batch 200/499 - Loss: 540.3967\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 134.1694\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 434.21263 Batch 405/499 - Loss: 348.4095\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 4/499 - Loss: 287.335442\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 425.76267 Batch 281/499 - Loss: 264.6667\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 230.2993\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 189.5836  Batch 495/499 - Loss: 327.9008\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 391.9733\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 279.91543 Batch 358/499 - Loss: 332.3389\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 232.0030\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 429.4904  Batch 464/499 - Loss: 438.8416\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 123/499 - Loss: 778.5127  Batch 484/499 - Loss: 154.0679\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 586.4055  Batch 334/499 - Loss: 497.7616\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 349.99333 Batch 21/499 - Loss: 546.9165\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 874.73666\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 341/499 - Loss: 564.96675\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 483.1618  Batch 188/499 - Loss: 257.9536\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 141.39160\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 308.20680 Batch 17/499 - Loss: 165.7507\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 380.6358  Batch 325/499 - Loss: 269.8790\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 308.62942 Batch 86/499 - Loss: 335.2634\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 364.17757 Batch 431/499 - Loss: 233.1287\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 164.0424\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 460.54044 Batch 263/499 - Loss: 269.5713\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 248.3013\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 102.2331  Batch 453/499 - Loss: 153.5068\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 385.3707  Batch 41/499 - Loss: 251.0878\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 117.69529 Batch 398/499 - Loss: 398.8448\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 872.8511 Batch 494/499 - Loss: 482.2864\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 303.49611 Batch 354/499 - Loss: 252.7988\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 568.9361\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 295.23105 Batch 339/499 - Loss: 471.5885\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 75/499 - Loss: 296.098708\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 587.69041 Batch 455/499 - Loss: 398.4030\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 93.21000\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 468.88245 Batch 215/499 - Loss: 331.8023\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 409.3345\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 387/499 - Loss: 337.68563 Batch 244/499 - Loss: 167.8135\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 63.54271 Batch 466/499 - Loss: 222.4697\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 266.20901\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 132.3392\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 111.23928 Batch 78/499 - Loss: 392.9413\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 412.89382\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 129.5393  Batch 465/499 - Loss: 231.1247\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 640.2557 Batch 473/499 - Loss: 299.7017\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 145.4549  Batch 256/499 - Loss: 207.0536\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 586.9643\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 69.997869\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 141.7950  Batch 136/499 - Loss: 181.4570\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 154.03588 Batch 290/499 - Loss: 250.2104\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 309.8783  Batch 93/499 - Loss: 142.0759\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 160.4115  Batch 399/499 - Loss: 102.6318\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 199.9348 Batch 468/499 - Loss: 452.6223\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 364.6438  Batch 331/499 - Loss: 324.8245\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 249.35269 Batch 12/499 - Loss: 319.9437\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 317.9216  Batch 421/499 - Loss: 348.6837\n",
      "Training Complete.\n",
      "\n",
      "best loss 54.50231170654297\n",
      "  Batch 499/499 - Loss: 126.6946\n",
      "--- Starting Epoch 29/32 ---\n",
      "6 layers, attempt #5ss: 266.72195\n",
      "  Batch 499/499 - Loss: 349.3857\n",
      "--- Starting Epoch 30/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "  Batch 96/499 - Loss: 337.2401\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c604450680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c6281dd090>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Epoch 1/32 ---4427\n",
      "  Batch 499/499 - Loss: 283.2577  Batch 72/499 - Loss: 1059.0686\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 321.26257 Batch 268/499 - Loss: 118.8687\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 22/499 - Loss: 445.79398 Batch 465/499 - Loss: 133.7637\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 115.80476 Batch 239/499 - Loss: 426.48837\n",
      "Training Complete.\n",
      "\n",
      "best loss 33.01231384277344\n",
      "  Batch 499/499 - Loss: 242.5036\n",
      "--- Starting Epoch 3/32 ---\n",
      "6 layers, attempt #6ss: 572.47493\n",
      "  Batch 375/499 - Loss: 389.3263\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7609147a9940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608b4dec310>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.2214\n",
      "  Batch 499/499 - Loss: 296.47984\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 399.5470  Batch 489/499 - Loss: 344.8564\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 272.47328 Batch 434/499 - Loss: 397.6193\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 145.6577 Batch 486/499 - Loss: 528.7729\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 435.05583 Batch 212/499 - Loss: 586.9904\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 270.4927\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 548.2934  Batch 405/499 - Loss: 119.9724\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 44/499 - Loss: 534.00900\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 103.97699 Batch 381/499 - Loss: 341.3638\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 706.8011\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 258.3483  Batch 350/499 - Loss: 387.5873\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 309.7063\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 137.0085  Batch 391/499 - Loss: 445.1534\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 97.39760 Batch 425/499 - Loss: 337.5022\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 801.9688  Batch 467/499 - Loss: 275.8181 Batch 56/499 - Loss: 409.9871\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 268.8430\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 195.91272 Batch 282/499 - Loss: 674.2743\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 255.0806\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 410/499 - Loss: 338.2672  Batch 490/499 - Loss: 303.1460\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 140/499 - Loss: 305.89029\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 174.92274 Batch 114/499 - Loss: 154.0054\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 313.1005\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 77.980671 Batch 166/499 - Loss: 253.9038\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 442.4677  Batch 392/499 - Loss: 321.7551\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 432.1061\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 498.62808 Batch 184/499 - Loss: 303.4112\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 120/499 - Loss: 202.4865\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 366.39357 Batch 375/499 - Loss: 336.4020\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 231.4623Batch 467/499 - Loss: 118.2304\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 491/499 - Loss: 313.36076 Batch 376/499 - Loss: 1140.4598\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 85.26837\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 109.7767  Batch 218/499 - Loss: 478.7581\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 68.48996\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 111.13921 Batch 449/499 - Loss: 199.2518\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 138.8616\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 484.44013 Batch 257/499 - Loss: 468.5564\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 246.8411 Batch 492/499 - Loss: 311.6688\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 418.64855 Batch 261/499 - Loss: 519.1759\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 714.0241\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 412.67507 Batch 415/499 - Loss: 556.7681\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 259.3665\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 479/499 - Loss: 209.43576 Batch 102/499 - Loss: 128.7207\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 120.22299\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 682.7964  Batch 326/499 - Loss: 261.2855\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 77.41199\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 146.82652Batch 68/499 - Loss: 456.5690\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 332.3977\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 415/499 - Loss: 194.23431 Batch 311/499 - Loss: 350.5578\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 900.5832\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 310.81646 Batch 262/499 - Loss: 509.7263\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 214.4177\n",
      "--- Starting Epoch 26/32 ---544\n",
      "  Batch 499/499 - Loss: 215.5898  Batch 411/499 - Loss: 268.6077\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 387.1479\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 453.3617  Batch 394/499 - Loss: 274.6111\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 65.571976\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 74.33471  Batch 368/499 - Loss: 279.1792\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 89.05739\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 160.38807atch 96/499 - Loss: 407.7735\n",
      "Training Complete.\n",
      "\n",
      "best loss 27.20638656616211\n",
      "  Batch 499/499 - Loss: 137.1856\n",
      "--- Starting Epoch 30/32 ---\n",
      "7 layers, attempt #1ss: 132.5200\n",
      "  Batch 406/499 - Loss: 301.7720\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.4211\n",
      "  Batch 499/499 - Loss: 206.54264\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 45/499 - Loss: 120.1112\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c6044528e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5794dddd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 51.410472 Batch 153/499 - Loss: 1690.8633\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 702.98945\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 69.66192\n",
      "Training Complete.\n",
      "\n",
      "best loss 40.87704086303711\n",
      "7 layers, attempt #2ss: 532.51759\n",
      "  Batch 407/499 - Loss: 428.8430\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608f45704a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608e4202590>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "  Batch 499/499 - Loss: 423.0062\n",
      "--- Starting Epoch 3/32 ---\n",
      "--- Starting Epoch 1/32 ---6496\n",
      "  Batch 499/499 - Loss: 149.5159  Batch 198/499 - Loss: 448.27525\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 849.9342  Batch 374/499 - Loss: 168.8986\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 212.3284\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 418.02567 Batch 243/499 - Loss: 433.0789\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 238.0957\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 354.7698  Batch 468/499 - Loss: 345.4997\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 1021.5828 Batch 86/499 - Loss: 292.4263\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 501.94625\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 126.0923  Batch 475/499 - Loss: 406.0394\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 102.89526 Batch 158/499 - Loss: 218.4702\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 85.71269  Batch 490/499 - Loss: 187.7202\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 515.46199\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 130/499 - Loss: 412.9531\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 328/499 - Loss: 533.0159  Batch 394/499 - Loss: 247.9027\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 144/499 - Loss: 446.9637  Batch 128/499 - Loss: 363.3522\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 419/499 - Loss: 597.85754 Batch 437/499 - Loss: 283.8567\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 305.1100\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 165.0686  Batch 393/499 - Loss: 304.0482\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 40/499 - Loss: 206.72338 Batch 464/499 - Loss: 377.7469\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 223.67035 Batch 403/499 - Loss: 148.0977\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 121.8022\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 192.2328  Batch 494/499 - Loss: 195.2417\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 98.2607\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 473.6006  Batch 347/499 - Loss: 263.6482\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 508.9857\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 491/499 - Loss: 296.9777  Batch 343/499 - Loss: 373.0100\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 295.7706\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 80.312539 Batch 238/499 - Loss: 258.5383\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 92.14035\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 538.05835 Batch 210/499 - Loss: 163.0507\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 475.6150\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 114.58806 Batch 206/499 - Loss: 436.5852\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 143.9321\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 74.98606ch 165/499 - Loss: 159.9194201\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan.5672ch 23/499 - Loss: 240.4169\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 427.9652ch 420/499 - Loss: 69.1126  Batch 256/499 - Loss: 132.9707  Batch 82/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan.31452Batch 311/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 264/499 - Loss: nan61322  Batch 201/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan.5211 Batch 341/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 307.4456ch 470/499 - Loss: 153.0727  Batch 336/499 - Loss: 137.0118\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan.5472ch 150/499 - Loss: 222.6147\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 89.73565ch 470/499 - Loss: 273.9691\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan.69627\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 417.8362\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 252/499 - Loss: 158.6344ch 148/499 - Loss: 178.8325\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 666.2647ch 463/499 - Loss: 199.0995\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan.8531ch 151/499 - Loss: 177.0544\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 190.99488\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan.2915  Batch 428/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 257.7549ch 462/499 - Loss: 104.6918\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan.39339atch 233/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 116/499 - Loss: nan.1372\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan.6564ch 313/499 - Loss: 266.6141\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 141/499 - Loss: nan.86404\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 447/499 - Loss: 238.00122\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 391.7786\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan.4586ch 21/499 - Loss: 449.9096\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 141.51960\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan.9314ch 190/499 - Loss: 88.1559\n",
      "Training Complete.\n",
      "\n",
      "best loss 56.47869873046875\n",
      "  Batch 499/499 - Loss: 342.67311\n",
      "--- Starting Epoch 31/32 ---\n",
      "7 layers, attempt #3ss: 560.1075\n",
      "  Batch 398/499 - Loss: 339.8688\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.5404\n",
      "  Batch 499/499 - Loss: 334.3958\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 145/499 - Loss: 140.8794\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c62af8e3e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c62af31ad0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 130.19692 Batch 155/499 - Loss: 484.3568\n",
      "Training Complete.\n",
      "\n",
      "best loss 22.53656768798828\n",
      "7 layers, attempt #4ss: 479.74546\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608f4571260> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608b403f7d0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 1096.0138\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 222.91944 Batch 492/499 - Loss: 373.6630\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 556.85570 Batch 133/499 - Loss: 406.8168\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 268.5629 Batch 471/499 - Loss: 151.3189\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 440/499 - Loss: nan  Batch 499/499 - Loss: 282.3706.0193\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 81/499 - Loss: 473.0396  Batch 461/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 404/499 - Loss: nan.2537ch 464/499 - Loss: 344.1518\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan1246  Batch 496/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 338.2701ch 498/499 - Loss: 576.9334\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan.9664ch 171/499 - Loss: 495.9467\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 251.7749ch 497/499 - Loss: 266.4470\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 396/499 - Loss: 483.56220 Batch 349/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 292.0307\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan.6026\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 549.9285ch 466/499 - Loss: 559.4781\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan.8744  Batch 463/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 108/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 143/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 478/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 413/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 420/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 291/499 - Loss: nan  Batch 223/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 490/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 459/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 369/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 407/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 374/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 11/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 462/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 154/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 428/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 155/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 469/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 202/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 441/499 - Loss: nan  Batch 442/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 316/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 178/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 266/499 - Loss: nan  Batch 86/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 121/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 259/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 254/499 - Loss: nan Batch 360/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 224/499 - Loss: nan Batch 336/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 467/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 339/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 459/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 172/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 68/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 325/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 263/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 493/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 73/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 90.8785171508789\n",
      "7 layers, attempt #5ss: nan\n",
      "  Batch 460/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 228/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5cc0dafc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c579408650>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 50/499 - Loss: 2087.6396  Batch 441/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan6.0659h 186/499 - Loss: 1526.1721\n",
      "Training Complete.\n",
      "\n",
      "best loss 242.51992797851562\n",
      "  Batch 499/499 - Loss: 1683.1562\n",
      "--- Starting Epoch 2/32 ---\n",
      "7 layers, attempt #6s: 1246.2922\n",
      "  Batch 138/499 - Loss: 1246.7012\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---7.2334\n",
      "  Batch 258/499 - Loss: 1767.6963\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608b4e5ba60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608b4b47750>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 1644.9513\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 1254.7365  Batch 366/499 - Loss: 320.5144\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 330.49994\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 1001.9532\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 535.35989 Batch 370/499 - Loss: 885.5342\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 144.0511\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 742.93292 Batch 438/499 - Loss: 546.0547\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 205.7967  Batch 139/499 - Loss: 566.2006\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 1029.6823Batch 221/499 - Loss: 479.4974\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 356.85969\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 399/499 - Loss: 198.71245 Batch 333/499 - Loss: 389.4445\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 319.5752 Batch 461/499 - Loss: 174.2845\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 895.44394 Batch 376/499 - Loss: 339.7759\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 104/499 - Loss: 499.7390\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 92.27596  Batch 422/499 - Loss: 224.6262\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 27/499 - Loss: 259.95235\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 469/499 - Loss: 625.39055 Batch 195/499 - Loss: 568.1641\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 1064.5000\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 127.14800 Batch 411/499 - Loss: 548.2305\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 516.1051\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan.2593  Batch 476/499 - Loss: nan32.0000\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 368.92326 Batch 65/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan.67865\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 151.9060\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 376/499 - Loss: 241.0323ch 169/499 - Loss: 179.5649\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 299.89597\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 243/499 - Loss: 125.5000  Batch 439/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 291/499 - Loss: nan.31146\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan.4155ch 136/499 - Loss: 197.8937\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 234.07326 193/499 - Loss: 248.3356\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan.1855\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 97.636248 Batch 10/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 50/499 - Loss: 166.20214\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 353/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 390/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 93/499 - Loss: nann\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 93/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 425/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 350/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 15/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 111/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 411/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 48/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 246/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 374/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 417/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 260/499 - Loss: nan Batch 302/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 182/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 360/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 303/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 247/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 471/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 319/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 140.15150451660156\n",
      "8 layers, attempt #1ss: nan\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 47/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 218/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c6044500e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c6281bc450>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan.47168 Batch 359/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 518.23093\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 210/499 - Loss: 1055.0790  Batch 446/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 38.08018112182617\n",
      "  Batch 499/499 - Loss: 1013.2126\n",
      "--- Starting Epoch 3/32 ---\n",
      "8 layers, attempt #2ss: 1241.3673\n",
      "  Batch 332/499 - Loss: 760.86834\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---3.8821\n",
      "  Batch 499/499 - Loss: 1516.8977\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 14/499 - Loss: 1067.2495\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608b4e589a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760897b79f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 809.10460\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 1262.3179 Batch 462/499 - Loss: 1565.1855\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 146.88733\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 209/499 - Loss: 205.65328  Batch 55/499 - Loss: 368.5466\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 86.445096 Batch 173/499 - Loss: 982.6499\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 303/499 - Loss: 184.96106 Batch 408/499 - Loss: 1073.1716\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 101.67745\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 1155.3970  Batch 79/499 - Loss: 443.8859\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 639.49019\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 629.29216  Batch 257/499 - Loss: 237.6657\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 42.074600 Batch 166/499 - Loss: 1438.9689\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 698.93899 Batch 386/499 - Loss: 1216.9210\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 285.71614 Batch 124/499 - Loss: 954.4669\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 356/499 - Loss: 216.66787\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 808.02131 Batch 49/499 - Loss: 932.9265\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 870.82318061376.0000\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan4.9714\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 1297.6720h 460/499 - Loss: 1477.5913\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan6.8726  Batch 477/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 42/499 - Loss: nan2.64125\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan.16376  Batch 443/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 349/499 - Loss: nan2.7660 Batch 268/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan6.7904h 94/499 - Loss: 1093.7051\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 843.35167  Batch 131/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan.9922\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 597.30067  Batch 292/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 23/499 - Loss: 1556.3535\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 2116.1870h 493/499 - Loss: 1704.0457\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan.8406  Batch 479/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 1576.2512  Batch 167/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan.0055\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan8.1479 Batch 479/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 236/499 - Loss: nan1.2798 Batch 207/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan9.1526  Batch 422/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 287/499 - Loss: nan9.1781\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan.52868h 9/499 - Loss: 1422.3003\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 1018.2440 209/499 - Loss: 1179.5969\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 418/499 - Loss: 971.82621h 145/499 - Loss: 978.4816\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 666.65084 473/499 - Loss: 1724.0021\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 270/499 - Loss: 1191.8796h 183/499 - Loss: 737.68540\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 1588.7134h 466/499 - Loss: 854.6096\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan8.6812  Batch 447/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 631.40164  Batch 361/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan.0557\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 925.90661 Batch 102/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan3.2485\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 1527.4247  Batch 153/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---  Batch 252/499 - Loss: nan\n",
      "  Batch 499/499 - Loss: nan8.7957 Batch 299/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 1414.2339h 404/499 - Loss: 1173.4280\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan0.3760\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 609.60880  Batch 409/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan93997  Batch 494/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 790.11817h 416/499 - Loss: 1005.5568\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 54/499 - Loss: 1316.5227\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 2203.8748 Batch 249/499 - Loss: nann\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan4.2646 Batch 394/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 677.25094\n",
      "Training Complete.\n",
      "\n",
      "best loss 270.7188720703125\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "8 layers, attempt #3ss: nan\n",
      "  Batch 241/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 103/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5f4294540> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c6201e0590>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan4.4086 Batch 421/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 42.07455825805664\n",
      "  Batch 498/499 - Loss: 421.42617\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608b4040540> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608b4e3ab10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 layers, attempt #4\n",
      "  Batch 499/499 - Loss: 611.5696\n",
      "--- Starting Epoch 2/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.3831\n",
      "  Batch 499/499 - Loss: 326.15049\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 460.3937ch 498/499 - Loss: 319.93794.0000\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 451.95942h 223/499 - Loss: 631.6864\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 17/499 - Loss: 211.3415\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 127.27261h 299/499 - Loss: 302.8465\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan.4091ch 134/499 - Loss: 397.1466\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 609.3184\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan.3413ch 237/499 - Loss: 219.1273\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 260.85351\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan.4679\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 323.54476\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan1987\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 352.51073 Batch 263/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan.1899\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 132.2278  Batch 178/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan.0043ch 196/499 - Loss: 249.6310\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 139.2114  Batch 350/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan.0786\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 72.18613ch 325/499 - Loss: 201.8262\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan.0610  Batch 452/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan.1179  Batch 333/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 33/499 - Loss: nan.71957\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan.2435  Batch 486/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 76.83278\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 151.9404  Batch 253/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan2285\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 593.81455 Batch 109/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan.4513  Batch 494/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan.2606  Batch 289/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 382.2578\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 273/499 - Loss: nannan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 413/499 - Loss: nan  Batch 481/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 68/499 - Loss: nann  Batch 23/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 468/499 - Loss: nan  Batch 197/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 232/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 61/499 - Loss: nann  Batch 3/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 277/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 420/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 402/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 412/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 98/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 387/499 - Loss: nan  Batch 345/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 418/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 348/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 186/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 314/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 284/499 - Loss: nan  Batch 181/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 492/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 461/499 - Loss: nan  Batch 391/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 321/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 199/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 49.05278778076172\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "8 layers, attempt #5ss: nan\n",
      "  Batch 401/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 56/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c604450680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5783e8650>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan5507628032.0000\n",
      "Training Complete.\n",
      "\n",
      "best loss 1412.611572265625\n",
      "8 layers, attempt #6ss: nan\n",
      "  Batch 498/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608b40418a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608f453ff10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan.23249\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan.0289  Batch 60/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 330.1264\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan.2335  Batch 184/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 491.8604  Batch 106/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan.74673 Batch 433/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 773.2981  Batch 33/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 669.2395ch 415/499 - Loss: 469.1600\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan6805\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 213.4098ch 323/499 - Loss: 151.3669\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan.5012  Batch 466/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan.44856h 386/499 - Loss: 537.2245  Batch 246/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 78.459806\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 455/499 - Loss: 366.20891 Batch 160/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 37/499 - Loss: nan.80647  Batch 11/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan.53626 Batch 359/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 101.9678\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan.83397 Batch 209/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 1045.6548\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan.09969h 38/499 - Loss: 281.2852\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 265.6991\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan62896.00007/499 - Loss: 119.1407\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 436/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 115/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 365/499 - Loss: nan  Batch 257/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 457/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 50/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 402/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 350/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 430/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 379/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 324/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 38/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 221/499 - Loss: nan  Batch 100/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 278/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 142/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 239/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 405/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 495/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 261/499 - Loss: nan Batch 422/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 298/499 - Loss: nan  Batch 101/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 435/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 482/499 - Loss: nan  Batch 480/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 276/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 421/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 384/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 465/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 458/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 133/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 80/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 162/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 108/499 - Loss: nan  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 59.80635070800781\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "9 layers, attempt #1ss: nan\n",
      "  Batch 220/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 348/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76089718fe20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760897a9ab10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 970.3187866210938\n",
      "9 layers, attempt #2ss: nan791392.0000\n",
      "  Batch 466/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5f42e8ae0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c572848690>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan6.9557h 436/499 - Loss: 1101.2389\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 1750.7970\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 175/499 - Loss: 904.70045h 12/499 - Loss: 2276.3433\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 268.5612  Batch 360/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan.4549\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan.81895\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 491.2420\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 172/499 - Loss: 438.52221Batch 252/499 - Loss: nan\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 290/499 - Loss: nan.5899  Batch 243/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan.4036ch 252/499 - Loss: 276.9888\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 194/499 - Loss: nan19019  Batch 115/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan.01776\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 226.0580ch 402/499 - Loss: 219.5499\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 134.9054ch 305/499 - Loss: 404.0953\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan7983 Batch 477/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 182.6420  Batch 300/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan0521\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 215.6878  Batch 221/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan.6810\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 82.47492  Batch 161/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan.9389\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 201.20136\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan4988\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 92.69939ch 412/499 - Loss: 256.7045\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 246/499 - Loss: 246.7364ch 188/499 - Loss: 384.9378\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 79.53751  Batch 209/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan.9263  Batch 449/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan.1999  Batch 288/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 35/499 - Loss: nan2.2581\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan.4365ch 211/499 - Loss: 227.1389\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 359.4958  Batch 64/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan.2822ch 321/499 - Loss: 197.4257\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 286/499 - Loss: nan.4182ch 428/499 - Loss: 520.8281\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan.2115 Batch 369/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 455/499 - Loss: nan.0595  Batch 414/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan8170\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 245.93029 85/499 - Loss: 252.7627\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 375/499 - Loss: 392.85132\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 3273308416.000092.0000\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan7409152.0000  Batch 486/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 1207644160.0000  Batch 183/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan164096.0000  Batch 358/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 138/499 - Loss: nan575104.0000  Batch 30/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan570736.0000  Batch 327/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 164058944.0000  Batch 145/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan93760.00000\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 208/499 - Loss: nan94064.0000\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan20544.00009/499 - Loss: 27148288.0000\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 22318200.0000\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 349/499 - Loss: 11037810.0000\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 8246455.50000\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan5978.0000  Batch 369/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 3030173.0000\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan0615.5000 Batch 392/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 1134294.5000  Batch 409/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan85.50005  Batch 431/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 419494.718833/499 - Loss: 838803.8750\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan971.4375128/499 - Loss: 310320.1250\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 143067.2812  Batch 279/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan95.06252 Batch 448/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 313.21600341796875\n",
      "  Batch 499/499 - Loss: 56088.0156\n",
      "--- Starting Epoch 31/32 ---\n",
      "9 layers, attempt #3ss: 35470.4023\n",
      "  Batch 305/499 - Loss: 31333.2949\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---83.4023\n",
      "  Batch 467/499 - Loss: 21058.7109\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608b40416c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608e4217310>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 20710.0273\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 10752.3525\n",
      "Training Complete.\n",
      "\n",
      "best loss 61.190067291259766\n",
      "9 layers, attempt #4ss: 2361.4141\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5731c2660> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5721aed50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 2465.9333\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 3102.1006  Batch 463/499 - Loss: 1163.9958\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 261/499 - Loss: 1061.6057\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 462.21157\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 994.83450  Batch 204/499 - Loss: 993.30412\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 857.84973\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 1138.9796\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 299.45319 Batch 352/499 - Loss: 1410.6257\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 933.67159\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 2214.7402 Batch 170/499 - Loss: 1108.4761\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 732.0169\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 738.96648 Batch 367/499 - Loss: 947.66008\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 206.14345 Batch 322/499 - Loss: 451.5455\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 941.88781 Batch 340/499 - Loss: 1213.0880\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 334.6520\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 1333.20518.0000 131/499 - Loss: 833.8053\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 237/499 - Loss: 1614.2657h 152/499 - Loss: 2037.9324\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 1089.1591h 442/499 - Loss: 1246.8601\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan8.1553Batch 217/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 1169.2323  Batch 101/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan1.5007h 135/499 - Loss: 1243.5735\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 1847.7280h 444/499 - Loss: 949.3026n\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan.3707ch 25/499 - Loss: 1439.2576\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 1344.9865h 488/499 - Loss: 885.1110n\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan.96314h 104/499 - Loss: 1053.2048\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 439/499 - Loss: nan1.2299  Batch 414/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan.5111\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 874.37872\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan0.2598h 124/499 - Loss: 1062.8794\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 673.68460  Batch 100/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan6.6534h 192/499 - Loss: 1068.0601\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 1288.2319h 376/499 - Loss: 1389.1560\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan.1061\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 1219.0829h 330/499 - Loss: 1217.1135\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 45/499 - Loss: 1103.7568\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 1828.4277 Batch 207/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan1.3912h 26/499 - Loss: 1098.2452\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 303/499 - Loss: nan1.5304  Batch 184/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan6.7787  Batch 492/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 1113.1884  Batch 268/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan61206\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 1007.3588h 460/499 - Loss: 1201.8726\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan6.1544\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 2008.0094  Batch 88/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan7.4099\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 1559.0073 Batch 169/499 - Loss: nann\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 319/499 - Loss: 1238.8723  Batch 445/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 140/499 - Loss: nan3.6694\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan.03668h 191/499 - Loss: 1497.5515\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 1467.9005\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan5.6593  Batch 494/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 2105.2439188/499 - Loss: 1274.7863\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan3.8823h 266/499 - Loss: 1284.7310\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 1519.3394\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 2069.9070  Batch 336/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan8.4586h 78/499 - Loss: 1141.4619\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan.80694  Batch 455/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 1157.3373  Batch 214/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan2.2242\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 903.89728h 492/499 - Loss: 1718.1237\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan8.5109h 108/499 - Loss: 1169.9729\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1520.8507h 316/499 - Loss: 1136.3822\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan.99309  Batch 436/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 133.7576141357422\n",
      "  Batch 499/499 - Loss: 1999.2632\n",
      "Training Complete.\n",
      "\n",
      "best loss 459.1705322265625\n",
      "9 layers, attempt #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "9 layers, attempt #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x760896a8dd00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760914097310>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 84/499 - Loss: 1483.4177\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c572860900> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c571b01f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 497/499 - Loss: 446.21122 Batch 124/499 - Loss: 2133.4194\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 297.17849\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 586.32327 Batch 55/499 - Loss: 562.8697\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 500.2202\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 496.86335 Batch 432/499 - Loss: 332.7473\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 887.8640\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 514.9230  Batch 461/499 - Loss: 652.4880\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 317.81388\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 448.20386 Batch 321/499 - Loss: 656.0704\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 92/499 - Loss: 683.90729\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 693.22492 Batch 276/499 - Loss: 287.8479\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 148.1829\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 1063.4316 Batch 483/499 - Loss: 1347.6560\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 80.62260\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 1099.2853  Batch 422/499 - Loss: 345.6198\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 907.49732 Batch 152/499 - Loss: 669.3663\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 293/499 - Loss: 972.90750 Batch 272/499 - Loss: 735.0428\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 887.12105 Batch 422/499 - Loss: 1734.6111\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 108.63237  Batch 424/499 - Loss: 323.3316\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 1629.0343\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 99.453169 Batch 186/499 - Loss: 658.6039\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 983.59421\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 1108.5634 Batch 264/499 - Loss: 1184.8186\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 806.54927 Batch 244/499 - Loss: 1144.7490\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 1118.2137\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 365.43347\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 1021.9304\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 377/499 - Loss: 1055.0232  Batch 451/499 - Loss: 253.9465\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 145/499 - Loss: 618.33535  Batch 133/499 - Loss: 395.1811\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 257.42755 Batch 35/499 - Loss: 1091.0702\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 1207.7727  Batch 124/499 - Loss: 239.3186\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 644.27846  Batch 498/499 - Loss: 412.2356\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 1363.4146 Batch 417/499 - Loss: 1022.8454\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 96.817429  Batch 376/499 - Loss: 317.0604\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 1227.2118  Batch 120/499 - Loss: 220.7965\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 180.67855 Batch 118/499 - Loss: 1591.3765\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 1029.0869\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 1049.4323 Batch 303/499 - Loss: 519.41671\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 286.01304 Batch 254/499 - Loss: 1774.1848\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 858.51148 Batch 488/499 - Loss: 947.84319\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 208/499 - Loss: 970.36168\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 899.74410 Batch 474/499 - Loss: 944.69932\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 779.92730 Batch 293/499 - Loss: 363.5637\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 847.57620\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 405/499 - Loss: 362.33367  Batch 319/499 - Loss: 248.7269\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 192/499 - Loss: 1169.5153\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 194/499 - Loss: 416.39030  Batch 135/499 - Loss: 375.4888\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 323/499 - Loss: 1210.3098 Batch 313/499 - Loss: 1270.9813\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 1141.4719  Batch 186/499 - Loss: 206.4393\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 337.94016  Batch 430/499 - Loss: 349.2996\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 853.32424 Batch 451/499 - Loss: 1543.4735\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 331.84918 Batch 327/499 - Loss: 643.9513\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 765.35312  Batch 269/499 - Loss: 521.5552\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 755.07270  Batch 456/499 - Loss: 361.9080\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 1042.1281\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 354/499 - Loss: 1595.3584 Batch 3/499 - Loss: 887.1027\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 1281.7375\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 511.29999 Batch 486/499 - Loss: 585.6565  Batch 370/499 - Loss: 1052.6000\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 922.49808\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 249.09425 Batch 428/499 - Loss: 1182.7039\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 2803.3525\n",
      "Training Complete.\n",
      "\n",
      "best loss 85.46065521240234\n",
      "10 layers, attempt #1s: 366.42677\n",
      "  Batch 499/499 - Loss: 1338.6115\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 32/499 - Loss: 430.9819\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 379/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5728613a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c572891690>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 35.01321792602539\n",
      "10 layers, attempt #2s: 785.09003\n",
      "  Batch 498/499 - Loss: 484.6809\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x760896a8d1c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760895c501d0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 644.3885\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 805.99863 Batch 140/499 - Loss: 1068.0629\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 28.896268 Batch 293/499 - Loss: 963.62192\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 198/499 - Loss: 654.13315\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 658.1830  Batch 450/499 - Loss: 610.6072\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 308.64472\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan.6127  Batch 472/499 - Loss: nan137\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 341.2894\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 413/499 - Loss: 449.6420ch 372/499 - Loss: 255.2037\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 3030.3623 481/499 - Loss: 477.2259\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan.03922h 151/499 - Loss: 249.0873\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 377.3399\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 467/499 - Loss: nan.4337ch 474/499 - Loss: 247.0278\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan3389\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan933760.0000 59/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 398/499 - Loss: nan  Batch 273/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 479/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 101/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 324/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 412/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 267/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 347/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 493/499 - Loss: nan  Batch 202/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 497/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 86/499 - Loss: nann\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 164/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 39/499 - Loss: nann\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 408/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 409/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 294/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 418/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 351/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 291/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 51/499 - Loss: nann\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 469/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 310/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 324/499 - Loss: nan  Batch 393/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 381/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 59/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 82/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 400/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 134/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 379/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 265/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 218/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 488/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 407/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 28.896167755126953\n",
      "10 layers, attempt #3s: nan\n",
      "  Batch 486/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 211/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5f42e8b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5721d5210>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan2.2664\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan1.5173  Batch 48/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 1124.8450\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan.25237\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 341/499 - Loss: nan2.0468h 393/499 - Loss: 1462.1826\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan.4606  Batch 483/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 46.91814422607422\n",
      "10 layers, attempt #4s: 1050.5659\n",
      "  Batch 442/499 - Loss: 1873.2739\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "\n",
      "--- Starting Epoch 4/32 ---\n",
      "--- Starting Epoch 1/32 ---1956\n",
      "  Batch 67/499 - Loss: 772.34770\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608e427ee80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608e4200690>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 514.2630374500567040.0000\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 219.68810\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan.7092\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan14240.0000\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 65/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 443/499 - Loss: nan  Batch 231/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 218/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 132/499 - Loss: nan\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 394/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 126/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 397/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 34/499 - Loss: nann\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 370/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 425/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 438/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 458/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 172/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 37/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 347/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 417/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 344/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 447/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 3/499 - Loss: nanan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 328/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 117/499 - Loss: nan  Batch 19/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 492/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 338/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 22/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 155/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 376/499 - Loss: nan  Batch 149/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 118/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 470/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 491/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 131/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nanBatch 237/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 45/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 297/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 178/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 88.59857177734375\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "10 layers, attempt #5: nan\n",
      "  Batch 107/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 367/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5f42e8fe0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5731a1a10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan.0047ch 351/499 - Loss: 440.2988\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 1628.7894 Batch 77/499 - Loss: nan\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan.5877\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 673.36780h 236/499 - Loss: 450.8253\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan6071tch 5/499 - Loss: 416.2718\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan.89594 Batch 88/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 1458.91943359375\n",
      "  Batch 499/499 - Loss: 753.4080\n",
      "--- Starting Epoch 4/32 ---\n",
      "10 layers, attempt #6s: 729.11340\n",
      "  Batch 345/499 - Loss: 619.9927\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 458.61959\n",
      "--- Starting Epoch 5/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.8262\n",
      "  Batch 266/499 - Loss: 413.5274\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608e427ef20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76095c0cadd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 168.29923\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 201.63989  Batch 346/499 - Loss: 346.1825\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 820.23284\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 623.00569\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 337.82996 Batch 468/499 - Loss: 550.3337\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 413.94554 Batch 188/499 - Loss: 412.5352\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 983.00625 Batch 423/499 - Loss: 459.3530\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 277.65450\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 243.8566  Batch 412/499 - Loss: 283.2557\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 411.23269\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 133.3367  Batch 337/499 - Loss: 258.0488\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 59/499 - Loss: 415.962223\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 428/499 - Loss: 415.82712 Batch 412/499 - Loss: 378.80649\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 138.9302\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 902.46617 Batch 168/499 - Loss: 249.0431\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 1072.6422 Batch 74/499 - Loss: 368.3455\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 643.33034\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 417.3579\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 218.5160 Batch 131/499 - Loss: 186.7306\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 252/499 - Loss: 396.94955\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 492.6725  Batch 193/499 - Loss: 332.3701\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 541.0570  Batch 257/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 210/499 - Loss: 320.7842 Batch 386/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 281.1082ch 447/499 - Loss: 400.4694\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan908669096329216.0000\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 310/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 389/499 - Loss: nan  Batch 132/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 457/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 6/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 379/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 172/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 375/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 196/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 172/499 - Loss: nan  Batch 484/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 375/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 98/499 - Loss: nan  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 417/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 457/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 72/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 165/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 493/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 9/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 149/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 307/499 - Loss: nan Batch 200/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 187/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 119/499 - Loss: nan  Batch 118/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 76.42472839355469\n",
      "11 layers, attempt #1s: nan\n",
      "  Batch 466/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 247/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5728627a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c57371fb90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan4653\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 58/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 59/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 38/499 - Loss: nann\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 309/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 428/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 94.72119903564453\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 224/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 layers, attempt #2s: nan\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 111/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608962e7b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760897b7ab10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 69/499 - Loss: 1335.96818\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan7.1917h 406/499 - Loss: 1029.0264\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 486/499 - Loss: nan1.9973 494/499 - Loss: 1766.3838\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan1787\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 1226.9048 119/499 - Loss: 1851.0948\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan8.6394\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 372/499 - Loss: nan8.0830  Batch 80/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: nan0.3877\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 1858.6531h 397/499 - Loss: 749.4073\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan4.4819h 38/499 - Loss: 1068.0168\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 1456.2030\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 341/499 - Loss: 1076.7911  Batch 266/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 1307.4182\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan5.1993 Batch 470/499 - Loss: nan8\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 164/499 - Loss: nan.41641\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan7.4983  Batch 297/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 845.69356\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan4.6443h 214/499 - Loss: 1178.8298\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 210/499 - Loss: nan7.4052h 442/499 - Loss: 880.1751\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan8.5112  Batch 476/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 594.38942  Batch 266/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan.22417  Batch 421/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 779.24573\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan.39509h 159/499 - Loss: 1285.0291\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 1211.4425 Batch 97/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan4.9390 Batch 291/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 727.05329h 446/499 - Loss: 1268.4564\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan7.8407h 111/499 - Loss: 1022.7372\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 1098.2212 Batch 490/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan1730\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 1033.3831h 367/499 - Loss: 1450.1089\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan.5886ch 58/499 - Loss: 1361.3688\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 1035.3923  Batch 318/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan.05298h 455/499 - Loss: 1095.0493\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 965.32324\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 1751.7734 Batch 448/499 - Loss: nann\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan.2314\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 480.41751\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan4.9589\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 1896.5070  Batch 441/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 43/499 - Loss: 1228.7983\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 959.60395h 306/499 - Loss: 869.4971\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan8.2939 Batch 464/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 239/499 - Loss: nan7.0493 253/499 - Loss: 766.0521n\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 266/499 - Loss: 1368.2977h 65/499 - Loss: 792.4241\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 697.56152 Batch 239/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan5.0510\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 177/499 - Loss: nan.10603 352/499 - Loss: 931.3964\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 410/499 - Loss: 966.07314  Batch 498/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 1225.4836\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 406/499 - Loss: 1124.0125h 390/499 - Loss: 1036.2556\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1415.9180\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 962.23322h 410/499 - Loss: 939.5591\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan41612\n",
      "Training Complete.\n",
      "\n",
      "best loss 1055.3968505859375\n",
      "11 layers, attempt #3s: 1155.8630\n",
      "  Batch 424/499 - Loss: 1225.1971\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.89283\n",
      "  Batch 499/499 - Loss: 2044.7089\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 55/499 - Loss: 1080.7793\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c5726fd800> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c5cc0f1850>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 1584.9524\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 908.66936  Batch 148/499 - Loss: 1527.8198\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 1611.5059\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 939.24768  Batch 270/499 - Loss: 1420.2288\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 1692.8040\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1842.5485  Batch 281/499 - Loss: 1252.0448\n",
      "Training Complete.\n",
      "\n",
      "best loss 350.32843017578125\n",
      "  Batch 499/499 - Loss: 1179.9448\n",
      "--- Starting Epoch 4/32 ---\n",
      "11 layers, attempt #4s: 1076.1245\n",
      "  Batch 456/499 - Loss: 992.68900\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 1648.4430\n",
      "--- Starting Epoch 5/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.9697\n",
      "  Batch 210/499 - Loss: 778.64670\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x760895fb32e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608978b5510>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 419.09159\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 431/499 - Loss: nan.9143  Batch 408/499 - Loss: nan\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan.31070\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 504.66921 388/499 - Loss: 371.2665\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan.06143h 229/499 - Loss: 449.7750\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 553.18446\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan.91321\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 67/499 - Loss: nan0.8117\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan.1802  Batch 497/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 107/499 - Loss: nan58466  Batch 47/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan.86432 Batch 434/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 781.31927h 430/499 - Loss: 336.6356\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan.8177  Batch 414/499 - Loss: nan\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 358.7094  Batch 168/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan.1336\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 824.28127\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan.1880 Batch 416/499 - Loss: nan  Batch 67/499 - Loss: 410.2685\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan.75654h 267/499 - Loss: 473.2690\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 450.5945\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 416.9704  Batch 284/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan54756\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 217.5394  Batch 270/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan4855\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan.95950 Batch 394/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 35.36377\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 437/499 - Loss: 256.77341 Batch 278/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 75.348558\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan.8069ch 298/499 - Loss: 374.3858\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 150/499 - Loss: nan.7776  Batch 38/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 187.9864ch 498/499 - Loss: 143.1132\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan5477  Batch 422/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: 494.7353ch 458/499 - Loss: 323.1938\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan.2675\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 214.19642\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan4884\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 364.4330ch 380/499 - Loss: 372.5294\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan42562\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan.04276\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 228.6826\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan.0720ch 410/499 - Loss: 241.4885\n",
      "--- Starting Epoch 21/32 ---6731\n",
      "  Batch 499/499 - Loss: 568.6609\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 390.76128 Batch 335/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan.6098  Batch 377/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 214.0590  Batch 64/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 284/499 - Loss: 308.8804  Batch 433/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 220.9223\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan.1564  Batch 491/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 155/499 - Loss: nan.5643  Batch 150/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan.7426ch 166/499 - Loss: 729.2841\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 272.97610\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan1889\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 211.96397 136/499 - Loss: 498.4286\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan8345\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 180.9417  Batch 324/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 35.363685607910156\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "11 layers, attempt #5s: nan\n",
      "  Batch 428/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 222/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c57148c860> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c572140910>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan3.8875h 92/499 - Loss: 848.8868\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan.96936 Batch 287/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 318/499 - Loss: nan.4626\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan.5226ch 124/499 - Loss: 744.4603\n",
      "Training Complete.\n",
      "\n",
      "best loss 580.9671020507812\n",
      "  Batch 499/499 - Loss: 130.94000\n",
      "--- Starting Epoch 3/32 ---\n",
      "11 layers, attempt #6: 331.8388\n",
      "  Batch 127/499 - Loss: 321.7961\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.1996\n",
      "  Batch 318/499 - Loss: 442.6542\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608e4263c40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608b4b6b6d0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 779.4418\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 594.62120 Batch 100/499 - Loss: 1263.9044\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 109.10090 Batch 10/499 - Loss: 146.3089\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 1084.0828\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 351.85655Batch 394/499 - Loss: 292.4128\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 265/499 - Loss: 162.70398 Batch 338/499 - Loss: 1135.6997\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 278/499 - Loss: 1262.2349 Batch 127/499 - Loss: 1376.7095\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 995.65222\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 709.62418\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 917.20125  Batch 48/499 - Loss: 293.9986\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 1872.3380 Batch 455/499 - Loss: 940.8655\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 134.6010\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 1146.1707  Batch 109/499 - Loss: 254.2583\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 289/499 - Loss: 925.49727 Batch 157/499 - Loss: 1026.9988\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 1317.6198 Batch 463/499 - Loss: 1364.6804\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 229.18002  Batch 354/499 - Loss: 349.0606\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 1343.6171 Batch 476/499 - Loss: 947.52604\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 499.41146\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 941.33505Batch 473/499 - Loss: 1010.4999\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 270.49655 Batch 104/499 - Loss: 994.5503\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 936.70289  Batch 45/499 - Loss: 559.3093\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 123.30362 Batch 335/499 - Loss: 911.81238\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 671.52441Batch 475/499 - Loss: 1511.4294\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 230.64595 Batch 311/499 - Loss: 1351.6394\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 61/499 - Loss: 316.511207\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 1509.8829  Batch 458/499 - Loss: 315.5798\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 419.4455  Batch 493/499 - Loss: 195.1827\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 955.92568 Batch 467/499 - Loss: 1635.7992\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: 377.05491\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 1058.27668371584.00009 - Loss: 180.7938\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan.48878h 250/499 - Loss: 1334.5635\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 801.49775 Batch 197/499 - Loss: nan5\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan5.3594  Batch 454/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 896.95085  Batch 130/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan8.8986\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 1280.3595h 455/499 - Loss: 827.0060\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan.23692h 253/499 - Loss: 1113.9849\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 1601.2667\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 285/499 - Loss: 832.01903\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 910.14984h 406/499 - Loss: 930.8785\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan1.5033Batch 208/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 893.67445h 463/499 - Loss: 1024.9175\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 360/499 - Loss: 1320.1261h 240/499 - Loss: 1515.3442\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 1004.5709\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan.91778h 249/499 - Loss: 1263.1364\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 1245.7616\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan.51111\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 780.31859\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan6.8831 Batch 420/499 - Loss: nann\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 756.28315\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan6.9890\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 1335.4774\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan5.5472  Batch 356/499 - Loss: nan  Batch 357/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 1091.8636\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan4.4404 Batch 224/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 896.54372 274/499 - Loss: 1328.8347\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 203/499 - Loss: 1786.0638  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1645.9718  Batch 178/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan.61378\n",
      "Training Complete.\n",
      "\n",
      "best loss 51.75199508666992\n",
      "  Batch 499/499 - Loss: 1289.1487\n",
      "--- Starting Epoch 31/32 ---\n",
      "12 layers, attempt #1: 978.49579\n",
      "  Batch 202/499 - Loss: 1082.0347\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---8.8173\n",
      "  Batch 420/499 - Loss: 1029.5017\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c6201b9440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c6044c2b10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 478.25834\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1286.80852000.00007/499 - Loss: 530.1008\n",
      "Training Complete.\n",
      "\n",
      "best loss 461.27923583984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 layers, attempt #2\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 2/32 ---\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 170/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76089784bb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760895f6f010>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 148/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 456/499 - Loss: nan  Batch 475/499 - Loss: nan\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 359/499 - Loss: nan Batch 108/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 341/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 425/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 156/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 245/499 - Loss: nan  Batch 447/499 - Loss: nan\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 3/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 495/499 - Loss: nan\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 330/499 - Loss: nan  Batch 226/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 334/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 319/499 - Loss: nan Batch 272/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 461/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 255/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 222/499 - Loss: nan  Batch 111/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 90/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 451/499 - Loss: nan  Batch 354/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 28/499 - Loss: nann\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 358/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 489/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 69/499 - Loss: nann Batch 464/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 296/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 138/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 161/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 222/499 - Loss: nan  Batch 462/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 389/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 86/499 - Loss: nann  Batch 66/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 27/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 255/499 - Loss: nan  Batch 256/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 123/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 190/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 347/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 53/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 48/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 318/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 6/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 333/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 151/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 64/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 403.15716552734375\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "12 layers, attempt #3 nan\n",
      "  Batch 85/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 273/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c6044a49a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c628163050>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan.47172h 125/499 - Loss: 524.3763\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan.68884 Batch 230/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 1042.2618\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan.69817 Batch 437/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 1033.0980224609375\n",
      "  Batch 499/499 - Loss: 342.07922\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 39/499 - Loss: 406.4214\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 layers, attempt #4: 342.3113\n",
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---.98782\n",
      "  Batch 241/499 - Loss: 331.7797\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608b40a89a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x760896b76f50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 58.947368\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: 619.42169h 480/499 - Loss: 116.9548.5815\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: inf.75141\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 78.10810  Batch 33/499 - Loss: inf\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: inf  Batch 401/499 - Loss: 4482000271198650368.0000\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 499/499 - Loss: 3683897216189071360.0000\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: inf9710195322257408.0000  Batch 220/499 - Loss: inf\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 35/499 - Loss: inf57266565262737408.0000\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: inf195800209620992.0000ss: 575246891466358784.0000\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 500060774530547712.0000\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: inf232554035478528.0000ss: 263594031424995328.0000\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: 184238497677180928.0000\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 67879226514079744.0000  Batch 443/499 - Loss: inf0\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: inf072443334656.0000\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 25008898779906048.0000oss: 34931853781630976.0000\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: inf\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 9214059912101888.0000  Batch 465/499 - Loss: inf0\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: inf\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 461/499 - Loss: inf4755304095744.0000Loss: 3729516597870592.0000\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: inf654622367744.0000\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 359/499 - Loss: inf0735815655424.0000Loss: 2216697582845952.0000\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: inf429948674048.00000\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 460811638145024.0000\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: inf769840640000.0000\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: 169777792286720.0000  Batch 233/499 - Loss: inf\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: inf555210764288.0000\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 62551530405888.0000  Batch 248/499 - Loss: inf\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: inf52751230976.0000- Loss: 36441946062848.0000\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: 23045894832128.0000  Batch 196/499 - Loss: inf\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 314/499 - Loss: 12294698303488.0000  Batch 408/499 - Loss: inf\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 170/499 - Loss: inf0918608896.0000  Batch 79/499 - Loss: inf0\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: inf7834741760.0000  Batch 318/499 - Loss: inf\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 202/499 - Loss: inf8265015296.0000- Loss: 3708314714112.0000\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: inf1591200768.0000 - Loss: 2239663964160.0000\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: 1152599851008.0000\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 325/499 - Loss: 601510379520.00009 - Loss: 696098816000.0000\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: 424636481536.0000  Batch 88/499 - Loss: inf\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: inf966765568.00009 - Loss: 240551723008.0000\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: 156463431680.0000\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: inf52536832.0000  Batch 494/499 - Loss: inf0\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: 57646583808.0000\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 13338148627167816981701827311586770944.0000\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: 21233164288.0000  Batch 51/499 - Loss: inf\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: 4914212029450110887666814172880961536.000064407337752526990415212755646676992.0000\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: 7824254976.0000828490104226732572672.0000\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: 1810553661040974348422703770658406400.0000  Batch 330/499 - Loss: 4041618688.0000\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: 2882763776.000011620172521956966400.0000\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: 667066188750436931964761308947021824.00000  Batch 273/499 - Loss: 1669797632.0000\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: 1062548224.000075696058728226750464.0000\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: 245768671754220374429579896682446848.0000893793345194167865531005946298368.0000\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: 391621504.0000490455645617179852800.0000127178780412632960893151120195584.0000\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 477/499 - Loss: 94624155387808126430324980099055616.00000  Batch 455/499 - Loss: 157260160.0000\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 90548945910163483112736855890591744.0000\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 494/499 - Loss: 33696549605982393987994575903391744.000075116928644611114802976640729088.00000\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: 33361095090016920211862961019420672.0000\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 469/499 - Loss: 13051747911110399913133904790290432.000076860880580077420597778359255040.0000\n",
      "Training Complete.\n",
      "\n",
      "best loss 58.94729995727539\n",
      "  Batch 499/499 - Loss: 12291312293478372944770245257592832.0000\n",
      "--- Starting Epoch 28/32 ---\n",
      "12 layers, attempt #5s: 4741808311843808997079223950639104.00000\n",
      "  Batch 499/499 - Loss: 4528520527625232616066166590275584.0000\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 26/499 - Loss: 4298943618884729380050917828591616.0000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...6492152832.0000\n",
      "--- Starting Epoch 1/32 ---2123491897094879721597221994496.0000\n",
      "  Batch 306/499 - Loss: 2454908600592741654578785471692800.0000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x76c6044a4fe0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x76c578331d90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: 1668452411789965456871952522149888.000031267888946861439975424000.0000\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: 614711235108164835626973990486016.0000  Batch 352/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: 226479331305640734724582813990912.0000\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan91651185232913734425462702080.0000\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: 83442187779244227385567750389760.00000  Batch 281/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 383.04071044921875\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 3/32 ---\n",
      "12 layers, attempt #6s: nan\n",
      "  Batch 256/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3420585/2545894807.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "/home/julian/StudioProjects/wade-datascience/.venv/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start: Resetting model weights to random...\n",
      "--- Starting Epoch 1/32 ---\n",
      "  Batch 464/499 - Loss: nan\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7608f44bd800> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Import object at 0x7608b4b35290>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: nan3.7744h 259/499 - Loss: 1276.2351\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: 1275.3300h 484/499 - Loss: 1256.6343\n",
      "--- Starting Epoch 2/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 499/499 - Loss: nan.95874h 205/499 - Loss: 458.4110\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: 461.6823h 475/499 - Loss: 313.3132\n",
      "--- Starting Epoch 3/32 ---\n",
      "  Batch 494/499 - Loss: nan.91585h 313/499 - Loss: 221.7998\n",
      "--- Starting Epoch 4/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: 148.7699  Batch 340/499 - Loss: nan\n",
      "--- Starting Epoch 5/32 ---\n",
      "  Batch 499/499 - Loss: nan8475\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: 104.56614 Batch 365/499 - Loss: nan\n",
      "--- Starting Epoch 6/32 ---\n",
      "  Batch 48/499 - Loss: 203.1900\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: 135.63948\n",
      "--- Starting Epoch 7/32 ---\n",
      "  Batch 499/499 - Loss: nan.5446ch 47/499 - Loss: 270.3662\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: 568.2916ch 388/499 - Loss: 160.7909\n",
      "--- Starting Epoch 8/32 ---\n",
      "  Batch 499/499 - Loss: nan.3369  Batch 469/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 276/499 - Loss: nan.2831\n",
      "--- Starting Epoch 9/32 ---\n",
      "  Batch 499/499 - Loss: nan.77491  Batch 473/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: 511.5310  Batch 162/499 - Loss: nan\n",
      "--- Starting Epoch 10/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 272/499 - Loss: nan02560.0000\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 11/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 326/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 12/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 498/499 - Loss: nan\n",
      "--- Starting Epoch 13/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 396/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 14/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 447/499 - Loss: nan\n",
      "--- Starting Epoch 15/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 16/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 10/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 127/499 - Loss: nan\n",
      "--- Starting Epoch 17/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 452/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 368/499 - Loss: nan\n",
      "--- Starting Epoch 18/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 312/499 - Loss: nan\n",
      "--- Starting Epoch 19/32 ---\n",
      "  Batch 499/499 - Loss: nan Batch 429/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 142/499 - Loss: nan\n",
      "--- Starting Epoch 20/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 465/499 - Loss: nan\n",
      "--- Starting Epoch 21/32 ---\n",
      "  Batch 302/499 - Loss: nan  Batch 491/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 22/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 249/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 91/499 - Loss: nan\n",
      "--- Starting Epoch 23/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 411/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 28/499 - Loss: nan\n",
      "--- Starting Epoch 24/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 396/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 25/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 456/499 - Loss: nan  Batch 408/499 - Loss: nan\n",
      "--- Starting Epoch 26/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 441/499 - Loss: nan\n",
      "--- Starting Epoch 27/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 240/499 - Loss: nan\n",
      "--- Starting Epoch 28/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 304/499 - Loss: nan\n",
      "--- Starting Epoch 29/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 477/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 295/499 - Loss: nan\n",
      "--- Starting Epoch 30/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 146/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 254/499 - Loss: nan\n",
      "--- Starting Epoch 31/32 ---\n",
      "  Batch 499/499 - Loss: nan  Batch 195/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 331.31951904296875\n",
      "  Batch 499/499 - Loss: nan\n",
      "--- Starting Epoch 32/32 ---\n",
      "  Batch 499/499 - Loss: nan\n",
      "Training Complete.\n",
      "\n",
      "best loss 53.791160583496094\n",
      "\n",
      "\n",
      "Overall Best RMSE 13.081047360545261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ModelArchitecture at 0x7c28a24b6690>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = find_best_model(\n",
    "    X = X,\n",
    "    y = y,\n",
    "    num_tries = 6\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
